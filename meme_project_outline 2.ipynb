{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "007c5775",
   "metadata": {},
   "source": [
    "\n",
    "# Meme Virality Across Platforms: Notebook\n",
    "**Group 23: Hanna , Lingchen Chen, Feihong Tian, Akshara Bruno** \n",
    "\n",
    "**Research question:**  \n",
    "*How do the visual template (meme format) and caption framing jointly influence cross-platform virality and sentiment?*\n",
    "\n",
    "This notebook is designed to:\n",
    "\n",
    "1. Collect meme data from Reddit and Bluesky.  \n",
    "2. Preprocess images and captions.  \n",
    "3. Cluster meme templates based on image content.  \n",
    "4. Analyze caption framing and sentiment.  \n",
    "5. Model virality as a function of template, caption, and platform.  \n",
    "6. Produce the main figures and tables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd609c2a",
   "metadata": {},
   "source": [
    "\n",
    "## 0. How to run this notebook\n",
    "\n",
    "1. Create a folder structure like:\n",
    "\n",
    "```\n",
    "project_root/\n",
    "  data/\n",
    "    raw/\n",
    "    processed/\n",
    "  images/\n",
    "  models/\n",
    "  notebook.ipynb\n",
    "```\n",
    "\n",
    "2. Place the cleaned datasets used in the paper into `data/processed/`  \n",
    "   (e.g., `reddit_memes.csv`, `bluesky_memes.csv`).\n",
    "\n",
    "3. (Optional) To re-scrape data from the APIs, follow the instructions in Section 2.\n",
    "\n",
    "4. Run all cells from top to bottom.\n",
    "\n",
    "If some external APIs or models cannot be accessed (e.g., CLIP model download),  \n",
    "the corresponding cells are clearly marked so you can skip them and still see the precomputed results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53bad7d",
   "metadata": {},
   "source": [
    "## 1. Imports and global configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa77f08-58b0-4064-9c27-e457a6028ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/aksharabruno/Documents/M Sc/VU/Social Web/Meme Analysis Project/.venv/bin/pip: line 2: /Users/aksharabruno/Documents/M Sc/VU/Social Web/Meme Project/.venv/bin/python: No such file or directory\n",
      "/Users/aksharabruno/Documents/M Sc/VU/Social Web/Meme Analysis Project/.venv/bin/pip: line 2: exec: /Users/aksharabruno/Documents/M Sc/VU/Social Web/Meme Project/.venv/bin/python: cannot execute: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt #needs to be changed. modules from requirements.txt should be added manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9934df98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aksharabruno/Documents/M Sc/VU/Social Web/Meme Analysis Project/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/aksharabruno/Documents/M Sc/VU/Social Web/Meme Analysis Project\n"
     ]
    }
   ],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Data & analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Machine learning / embeddings / clustering\n",
    "try:\n",
    "    from sklearn.cluster import KMeans\n",
    "    from sklearn.manifold import TSNE\n",
    "except ImportError:\n",
    "    print(\"scikit-learn is not installed. Install it to enable clustering and t-SNE visualizations.\")\n",
    "\n",
    "# Deep learning / CLIP (for image embeddings)\n",
    "try:\n",
    "    import torch\n",
    "    from PIL import Image\n",
    "    from transformers import CLIPProcessor, CLIPModel\n",
    "except ImportError:\n",
    "    print(\"torch / transformers / pillow not installed. Install them to enable image embeddings.\")\n",
    "\n",
    "# NLP\n",
    "try:\n",
    "    import nltk\n",
    "    from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "except ImportError:\n",
    "    print(\"nltk not installed. Install it to enable VADER sentiment analysis.\")\n",
    "\n",
    "# Global paths\n",
    "PROJECT_ROOT = Path('.').resolve()\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "RAW_DIR = DATA_DIR / \"raw\"\n",
    "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
    "IMAGES_DIR = PROJECT_ROOT / \"images\"\n",
    "\n",
    "for p in [DATA_DIR, RAW_DIR, PROCESSED_DIR, IMAGES_DIR]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Project root:\", PROJECT_ROOT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9875f57",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Data collection (overview)\n",
    "\n",
    "We collect meme posts from:\n",
    "\n",
    "- **Reddit** ‚Äì using the Reddit API (e.g. PRAW or Pushshift-like interface).  \n",
    "- **Bluesky** ‚Äì using the ATProto API.\n",
    "\n",
    "For each platform we aim to obtain:\n",
    "\n",
    "- Post ID, author, timestamp  \n",
    "- Caption / title text  \n",
    "- Image URL(s)  \n",
    "- Engagement statistics (upvotes / likes, comments / replies, reposts)  \n",
    "- Platform indicator and, for Reddit, subreddit information  \n",
    "\n",
    "In practice, the actual scraping code may be placed in a separate script to avoid  \n",
    "API credential issues inside the notebook. Here we provide a minimal example skeleton.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b950828d",
   "metadata": {},
   "source": [
    "### 2.1 Reddit scraping skeleton (to run once, offline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a383d28c",
   "metadata": {},
   "source": [
    "Getting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92d61bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k5/sv2gpgh93ks3whjc1lnjkh380000gn/T/ipykernel_97541/2584464678.py:25: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  current_time_iso = datetime.datetime.utcnow().isoformat()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Reddit data collection...\n",
      "   Collecting from Feb 2024 onwards, minimum score: 50\n",
      "\n",
      "   Processing r/memes...\n",
      "      - Fetching top posts from past year...\n",
      "      - Fetching top posts from past month...\n",
      "      Collected 453 posts from r/memes\n",
      "\n",
      "   Processing r/dankmemes...\n",
      "      - Fetching top posts from past year...\n",
      "      - Fetching top posts from past month...\n",
      "      Collected 467 posts from r/dankmemes\n",
      "\n",
      "   Processing r/AdviceAnimals...\n",
      "      - Fetching top posts from past year...\n",
      "      - Fetching top posts from past month...\n",
      "      Collected 480 posts from r/AdviceAnimals\n",
      "\n",
      "   Processing r/MemeEconomy...\n",
      "      - Fetching top posts from past year...\n",
      "      - Fetching top posts from past month...\n",
      "      Collected 145 posts from r/MemeEconomy\n",
      "\n",
      "   Processing r/me_irl...\n",
      "      - Fetching top posts from past year...\n",
      "      - Fetching top posts from past month...\n",
      "      Collected 439 posts from r/me_irl\n",
      "\n",
      "‚úÖ Total unique posts collected: 1984\n",
      "‚úÖ Saved to data/raw/reddit_memes_raw.csv\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "from time import sleep\n",
    "\n",
    "# Configuration\n",
    "date_feb_2024_utc = datetime.datetime(2024, 2, 1, tzinfo=datetime.timezone.utc)\n",
    "TIMESTAMP_FEB_2024 = date_feb_2024_utc.timestamp()\n",
    "MIN_SCORE = 50  # Adjust based on what you consider \"viral\"\n",
    "\n",
    "RAW_DIR = Path(\"data/raw\")\n",
    "reddit_path = RAW_DIR / \"reddit_memes_raw.csv\"\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"Dhu5BDkroWpRBQ\",\n",
    "    client_secret=\"ojQeyIZ793E7I6fRFILOiLdJzdGNcg\",\n",
    "    user_agent=\"meme-research-notebook\"\n",
    ")\n",
    "\n",
    "# Target meme-heavy subreddits\n",
    "target_subreddits = [\"memes\", \"dankmemes\", \"AdviceAnimals\", \"MemeEconomy\", \"me_irl\"]\n",
    "\n",
    "posts = []\n",
    "current_time_iso = datetime.datetime.utcnow().isoformat()\n",
    "seen_ids = set()\n",
    "\n",
    "print(\"üöÄ Starting Reddit data collection...\")\n",
    "print(f\"   Collecting from Feb 2024 onwards, minimum score: {MIN_SCORE}\")\n",
    "\n",
    "for subreddit_name in target_subreddits:\n",
    "    print(f\"\\n   Processing r/{subreddit_name}...\")\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    \n",
    "    # Collect from multiple time filters to maximize coverage\n",
    "    for time_filter in ['year', 'month']:\n",
    "        print(f\"      - Fetching top posts from past {time_filter}...\")\n",
    "        \n",
    "        for submission in subreddit.top(time_filter=time_filter, limit=None):\n",
    "            # Skip if already seen\n",
    "            if submission.id in seen_ids:\n",
    "                continue\n",
    "                \n",
    "            # Apply date filter\n",
    "            if submission.created_utc < TIMESTAMP_FEB_2024:\n",
    "                continue\n",
    "                \n",
    "            # Apply score filter\n",
    "            if submission.score < MIN_SCORE:\n",
    "                continue\n",
    "            \n",
    "            # Only collect image posts\n",
    "            if not submission.url.endswith(('.jpg', '.png', '.gif', '.jpeg')):\n",
    "                continue\n",
    "            \n",
    "            seen_ids.add(submission.id)\n",
    "            \n",
    "            author_name = \"[deleted]\"\n",
    "            if submission.author is not None:\n",
    "                author_name = submission.author.name\n",
    "            \n",
    "            posts.append({\n",
    "                \"platform\": \"reddit\",\n",
    "                \"meme_template\": \"Unknown\",  # Will need manual tagging later\n",
    "                \"post_id\": submission.id,\n",
    "                \"subreddit\": subreddit_name,\n",
    "                \"author_handle\": author_name,\n",
    "                \"caption\": submission.title,\n",
    "                \"image_url\": submission.url,\n",
    "                \"likes\": submission.score,\n",
    "                \"upvote_ratio\": submission.upvote_ratio,\n",
    "                \"replies\": submission.num_comments,\n",
    "                \"created_at\": datetime.datetime.fromtimestamp(\n",
    "                    submission.created_utc, tz=datetime.timezone.utc\n",
    "                ).isoformat(),\n",
    "                \"indexed_at\": current_time_iso,\n",
    "                \"post_url\": f\"https://www.reddit.com{submission.permalink}\"\n",
    "            })\n",
    "        \n",
    "        sleep(2)  # Be nice to Reddit's API\n",
    "    \n",
    "    print(f\"      Collected {len([p for p in posts if p['subreddit'] == subreddit_name])} posts from r/{subreddit_name}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Total unique posts collected: {len(posts)}\")\n",
    "\n",
    "# Save to CSV\n",
    "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "reddit_df = pd.DataFrame(posts)\n",
    "reddit_df.to_csv(reddit_path, index=False)\n",
    "print(f\"‚úÖ Saved to {reddit_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad80c8e",
   "metadata": {},
   "source": [
    "Download the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9390976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 1984 images...\n",
      "   Downloaded 50/1984 images...\n",
      "   Downloaded 100/1984 images...\n",
      "   Downloaded 150/1984 images...\n",
      "   Downloaded 200/1984 images...\n",
      "   Downloaded 250/1984 images...\n",
      "   Downloaded 300/1984 images...\n",
      "   Downloaded 350/1984 images...\n",
      "   Downloaded 400/1984 images...\n",
      "   Downloaded 450/1984 images...\n",
      "   Downloaded 500/1984 images...\n",
      "   Downloaded 550/1984 images...\n",
      "   Downloaded 600/1984 images...\n",
      "   Downloaded 650/1984 images...\n",
      "   Downloaded 700/1984 images...\n",
      "   Downloaded 750/1984 images...\n",
      "   Downloaded 800/1984 images...\n",
      "   Downloaded 850/1984 images...\n",
      "   Downloaded 900/1984 images...\n",
      "   Downloaded 950/1984 images...\n",
      "   Downloaded 1000/1984 images...\n",
      "   Downloaded 1050/1984 images...\n",
      "   Downloaded 1100/1984 images...\n",
      "   Downloaded 1150/1984 images...\n",
      "   Downloaded 1200/1984 images...\n",
      "   Downloaded 1250/1984 images...\n",
      "   Downloaded 1300/1984 images...\n",
      "   Downloaded 1350/1984 images...\n",
      "   Downloaded 1400/1984 images...\n",
      "   Downloaded 1450/1984 images...\n",
      "   Downloaded 1500/1984 images...\n",
      "   Downloaded 1550/1984 images...\n",
      "   Downloaded 1600/1984 images...\n",
      "   Downloaded 1650/1984 images...\n",
      "   Downloaded 1700/1984 images...\n",
      "   Downloaded 1750/1984 images...\n",
      "   Downloaded 1800/1984 images...\n",
      "   Downloaded 1850/1984 images...\n",
      "   Downloaded 1900/1984 images...\n",
      "   Downloaded 1950/1984 images...\n",
      "‚úÖ Download complete!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "\n",
    "# Load your collected data\n",
    "df = pd.read_csv('data/raw/reddit_memes_raw.csv')\n",
    "\n",
    "# Create directory for images\n",
    "img_dir = Path('images/reddit_images')\n",
    "img_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Downloading {len(df)} images...\")\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    try:\n",
    "        # Download image\n",
    "        response = requests.get(row['image_url'], timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            # Get file extension\n",
    "            ext = row['image_url'].split('.')[-1].split('?')[0]  # Remove query params\n",
    "            if ext not in ['jpg', 'jpeg', 'png', 'gif']:\n",
    "                ext = 'jpg'\n",
    "            \n",
    "            # Save with post_id as filename\n",
    "            filepath = img_dir / f\"{row['post_id']}.{ext}\"\n",
    "            with open(filepath, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "            \n",
    "            # Update dataframe with local path\n",
    "            df.at[idx, 'image_path'] = str(filepath)\n",
    "            \n",
    "        if (idx + 1) % 50 == 0:\n",
    "            print(f\"   Downloaded {idx + 1}/{len(df)} images...\")\n",
    "            sleep(1)  # Be nice to servers\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download {row['post_id']}: {e}\")\n",
    "\n",
    "# Save updated dataframe\n",
    "df.to_csv('data/raw/reddit_memes_raw.csv', index=False)\n",
    "print(\"‚úÖ Download complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e72eff",
   "metadata": {},
   "source": [
    "### 2.2 Bluesky scraping skeleton (to run once, offline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2aa4ae7f-4957-488f-93cf-6903900dde46",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: atproto in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (0.0.64)\n",
      "Requirement already satisfied: cryptography<46,>=41.0.7 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from atproto) (45.0.7)\n",
      "Requirement already satisfied: websockets<16,>=15 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from atproto) (15.0.1)\n",
      "Requirement already satisfied: libipld<4,>=3.0.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from atproto) (3.3.0)\n",
      "Requirement already satisfied: dnspython<3,>=2.4.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from atproto) (2.7.0)\n",
      "Requirement already satisfied: httpx<0.29.0,>=0.25.0 in /Users/chenlingchen/Library/Python/3.9/lib/python/site-packages (from atproto) (0.28.1)\n",
      "Requirement already satisfied: click<9,>=8.1.3 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from atproto) (8.1.8)\n",
      "Requirement already satisfied: pydantic<3,>=2.7 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from atproto) (2.12.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.8.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from atproto) (4.15.0)\n",
      "Requirement already satisfied: cffi>=1.14 in /Users/chenlingchen/Library/Python/3.9/lib/python/site-packages (from cryptography<46,>=41.0.7->atproto) (2.0.0)\n",
      "Requirement already satisfied: idna in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from httpx<0.29.0,>=0.25.0->atproto) (3.4)\n",
      "Requirement already satisfied: anyio in /Users/chenlingchen/Library/Python/3.9/lib/python/site-packages (from httpx<0.29.0,>=0.25.0->atproto) (4.10.0)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/chenlingchen/Library/Python/3.9/lib/python/site-packages (from httpx<0.29.0,>=0.25.0->atproto) (1.0.9)\n",
      "Requirement already satisfied: certifi in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from httpx<0.29.0,>=0.25.0->atproto) (2025.11.12)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/chenlingchen/Library/Python/3.9/lib/python/site-packages (from httpcore==1.*->httpx<0.29.0,>=0.25.0->atproto) (0.16.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from pydantic<3,>=2.7->atproto) (0.4.2)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from pydantic<3,>=2.7->atproto) (2.41.5)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from pydantic<3,>=2.7->atproto) (0.7.0)\n",
      "Requirement already satisfied: pycparser in /Users/chenlingchen/Library/Python/3.9/lib/python/site-packages (from cffi>=1.14->cryptography<46,>=41.0.7->atproto) (2.23)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from anyio->httpx<0.29.0,>=0.25.0->atproto) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from anyio->httpx<0.29.0,>=0.25.0->atproto) (1.0.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install atproto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35536dd2-9657-4cc1-9d3b-304dd81ff1bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to login...\n",
      "\n",
      "Trying to login with: linnnnlnl.bsky.social\n",
      "‚ùå Failed: Response(success=False, status_code=401, content=XrpcError(error='AuthenticationRequired', message='Invalid identifier or password'), headers={'date': 'Mon, 08 Dec 2025 16:14:34 GMT', 'content-type': 'application/json; charset=utf-8', 'content-length': '77', 'connection': 'keep-alive', 'x-powered-by': 'Express', 'access-control-allow-origin': '*', 'ratelimit-limit': '10', 'ratelimit-remaining': '9', 'ratelimit-reset': '1765296874', 'ratelimit-policy': '10;w=86400', 'etag': 'W/\"4d-98r3hvgolnybv7tgksQiZbSE7Zg\"', 'vary': 'Accept-Encoding'})\n",
      "\n",
      "Trying to login with: treasureclc@gmail.com\n",
      "‚úÖ SUCCESS with treasureclc@gmail.com\n",
      "   Logged in as: linnninl.bsky.social\n",
      "\n",
      "üìä Found 3 posts for 'change my mind'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# explore post structure\\nfor i, post in enumerate(feed.posts):\\n    print(f\"\\n{\\'=\\'*60}\")\\n    print(f\"POST {i+1}\")\\n    print(\\'=\\'*60)\\n    \\n    # Basic info\\n    print(f\"URI (unique ID): {post.uri}\")\\n    print(f\"Author: {post.author.handle}\")\\n    \\n    # Text content\\n    if hasattr(post, \\'record\\'):\\n        text = post.record.text[:150]\\n        print(f\"Text: {text}...\" if len(post.record.text) > 150 else f\"Text: {post.record.text}\")\\n        print(f\"Created at: {post.record.created_at}\")\\n    \\n    # Check for images\\n    has_images = False\\n    if hasattr(post, \\'embed\\') and hasattr(post.embed, \\'images\\'):\\n        image_count = len(post.embed.images)\\n        print(f\"Has images: YES ({image_count} images)\")\\n        for img_idx, img in enumerate(post.embed.images[:2]):  # Show first 2\\n            if hasattr(img, \\'image\\'):\\n                print(f\"  Image {img_idx+1}: {img.image}\")\\n        has_images = True\\n    else:\\n        print(\"Has images: NO\")\\n    \\n    # Engagement metrics\\n    print(f\"Likes: {getattr(post, \\'like_count\\', 0)}\")\\n    print(f\"Reposts: {getattr(post, \\'repost_count\\', 0)}\")\\n    print(f\"Replies: {getattr(post, \\'reply_count\\', 0)}\")\\n    \\n    # Quick summary for our project\\n    print(f\"\\n‚úÖ FOR OUR PROJECT: This post has {\\'images\\' if has_images else \\'NO images\\'} and text for analysis\")\\n    '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step 1: log in\n",
    "print(\"Trying to login...\")\n",
    "\n",
    "from atproto import Client\n",
    "\n",
    "client = Client()\n",
    "\n",
    "# Try both options\n",
    "identifiers = [\n",
    "    \"linnnnlnl.bsky.social\",  # handle without @\n",
    "    \"treasureclc@gmail.com\",  # your email if handle doesn't work\n",
    "    \"@linnnnlnl.bsky.social\",  # handle with @ (less likely to work)\n",
    "]\n",
    "\n",
    "for identifier in identifiers:\n",
    "    print(f\"\\nTrying to login with: {identifier}\")\n",
    "    try:\n",
    "        session = client.login(identifier, \"qvip-timq-o322-hnzz\")\n",
    "        print(f\"‚úÖ SUCCESS with {identifier}\")\n",
    "        print(f\"   Logged in as: {session.handle}\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed: {e}\")\n",
    "\n",
    "# Now that we're logged in, let's understand post structure\n",
    "from atproto import Client, models\n",
    "\n",
    "# We already have a logged-in client \n",
    "# Let's test the search with the authenticated client\n",
    "params = models.AppBskyFeedSearchPosts.Params(\n",
    "    q=\"change my mind\",\n",
    "    limit=3  # Get 3 posts to see variety\n",
    ")\n",
    "\n",
    "feed = client.app.bsky.feed.search_posts(params)\n",
    "\n",
    "print(f\"\\nüìä Found {len(feed.posts)} posts for 'change my mind'\")\n",
    "\n",
    "'''\n",
    "# explore post structure\n",
    "for i, post in enumerate(feed.posts):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"POST {i+1}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    # Basic info\n",
    "    print(f\"URI (unique ID): {post.uri}\")\n",
    "    print(f\"Author: {post.author.handle}\")\n",
    "    \n",
    "    # Text content\n",
    "    if hasattr(post, 'record'):\n",
    "        text = post.record.text[:150]\n",
    "        print(f\"Text: {text}...\" if len(post.record.text) > 150 else f\"Text: {post.record.text}\")\n",
    "        print(f\"Created at: {post.record.created_at}\")\n",
    "    \n",
    "    # Check for images\n",
    "    has_images = False\n",
    "    if hasattr(post, 'embed') and hasattr(post.embed, 'images'):\n",
    "        image_count = len(post.embed.images)\n",
    "        print(f\"Has images: YES ({image_count} images)\")\n",
    "        for img_idx, img in enumerate(post.embed.images[:2]):  # Show first 2\n",
    "            if hasattr(img, 'image'):\n",
    "                print(f\"  Image {img_idx+1}: {img.image}\")\n",
    "        has_images = True\n",
    "    else:\n",
    "        print(\"Has images: NO\")\n",
    "    \n",
    "    # Engagement metrics\n",
    "    print(f\"Likes: {getattr(post, 'like_count', 0)}\")\n",
    "    print(f\"Reposts: {getattr(post, 'repost_count', 0)}\")\n",
    "    print(f\"Replies: {getattr(post, 'reply_count', 0)}\")\n",
    "    \n",
    "    # Quick summary for our project\n",
    "    print(f\"\\n‚úÖ FOR OUR PROJECT: This post has {'images' if has_images else 'NO images'} and text for analysis\")\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c8acfa7-f69b-4bf6-b2b6-827a4a2192ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ COMPREHENSIVE MEME DATA COLLECTION\n",
      "=====================================\n",
      "Collecting 10 meme templates at 2025-12-09 14:11:06\n",
      "\n",
      "üîç Searching for: 'change my mind'\n",
      "----------------------------------------\n",
      "  Query 1: 'change my mind meme'\n",
      "    Error with query 'change my mind meme': name 'models' is not defined\n",
      "  Query 2: 'changemymind'\n",
      "    Error with query 'changemymind': name 'models' is not defined\n",
      "  Query 3: '#changemymind'\n",
      "    Error with query '#changemymind': name 'models' is not defined\n",
      "üìä Total for 'change my mind': 0 meme posts\n",
      "\n",
      "üîç Searching for: 'drake hotline bling'\n",
      "----------------------------------------\n",
      "  Query 1: 'drake hotline bling meme'\n",
      "    Error with query 'drake hotline bling meme': name 'models' is not defined\n",
      "  Query 2: 'drake meme'\n",
      "    Error with query 'drake meme': name 'models' is not defined\n",
      "  Query 3: 'drake reaction'\n",
      "    Error with query 'drake reaction': name 'models' is not defined\n",
      "üìä Total for 'drake hotline bling': 0 meme posts\n",
      "\n",
      "üîç Searching for: 'distracted boyfriend'\n",
      "----------------------------------------\n",
      "  Query 1: 'distracted boyfriend meme'\n",
      "    Error with query 'distracted boyfriend meme': name 'models' is not defined\n",
      "  Query 2: 'distracted boyfriend'\n",
      "    Error with query 'distracted boyfriend': name 'models' is not defined\n",
      "  Query 3: 'looking at other girl'\n",
      "    Error with query 'looking at other girl': name 'models' is not defined\n",
      "üìä Total for 'distracted boyfriend': 0 meme posts\n",
      "\n",
      "üîç Searching for: 'two button choice'\n",
      "----------------------------------------\n",
      "  Query 1: 'two button meme'\n",
      "    Error with query 'two button meme': name 'models' is not defined\n",
      "  Query 2: 'two buttons'\n",
      "    Error with query 'two buttons': name 'models' is not defined\n",
      "  Query 3: 'which button'\n",
      "    Error with query 'which button': name 'models' is not defined\n",
      "üìä Total for 'two button choice': 0 meme posts\n",
      "\n",
      "üîç Searching for: 'expanding brain'\n",
      "----------------------------------------\n",
      "  Query 1: 'expanding brain meme'\n",
      "    Error with query 'expanding brain meme': name 'models' is not defined\n",
      "  Query 2: 'brain expanding'\n",
      "    Error with query 'brain expanding': name 'models' is not defined\n",
      "  Query 3: '5d chess'\n",
      "    Error with query '5d chess': name 'models' is not defined\n",
      "üìä Total for 'expanding brain': 0 meme posts\n",
      "\n",
      "üîç Searching for: 'woman yelling at cat'\n",
      "----------------------------------------\n",
      "  Query 1: 'woman yelling at cat meme'\n",
      "    Error with query 'woman yelling at cat meme': name 'models' is not defined\n",
      "  Query 2: 'yelling cat meme'\n",
      "    Error with query 'yelling cat meme': name 'models' is not defined\n",
      "  Query 3: 'woman cat restaurant'\n",
      "    Error with query 'woman cat restaurant': name 'models' is not defined\n",
      "üìä Total for 'woman yelling at cat': 0 meme posts\n",
      "\n",
      "üîç Searching for: 'surprised pikachu'\n",
      "----------------------------------------\n",
      "  Query 1: 'surprised pikachu meme'\n",
      "    Error with query 'surprised pikachu meme': name 'models' is not defined\n",
      "  Query 2: 'pikachu surprised'\n",
      "    Error with query 'pikachu surprised': name 'models' is not defined\n",
      "  Query 3: 'pikachu face'\n",
      "    Error with query 'pikachu face': name 'models' is not defined\n",
      "üìä Total for 'surprised pikachu': 0 meme posts\n",
      "\n",
      "üîç Searching for: 'mocking spongebob'\n",
      "----------------------------------------\n",
      "  Query 1: 'mocking spongebob meme'\n",
      "    Error with query 'mocking spongebob meme': name 'models' is not defined\n",
      "  Query 2: 'spongebob mock'\n",
      "    Error with query 'spongebob mock': name 'models' is not defined\n",
      "  Query 3: 'mockingspongebob'\n",
      "    Error with query 'mockingspongebob': name 'models' is not defined\n",
      "üìä Total for 'mocking spongebob': 0 meme posts\n",
      "\n",
      "üîç Searching for: 'this is fine'\n",
      "----------------------------------------\n",
      "  Query 1: 'this is fine meme'\n",
      "    Error with query 'this is fine meme': name 'models' is not defined\n",
      "  Query 2: 'dog burning house'\n",
      "    Error with query 'dog burning house': name 'models' is not defined\n",
      "  Query 3: 'everything is fine'\n",
      "    Error with query 'everything is fine': name 'models' is not defined\n",
      "üìä Total for 'this is fine': 0 meme posts\n",
      "\n",
      "üîç Searching for: 'is this a pigeon'\n",
      "----------------------------------------\n",
      "  Query 1: 'is this a pigeon meme'\n",
      "    Error with query 'is this a pigeon meme': name 'models' is not defined\n",
      "  Query 2: 'butterfly meme'\n",
      "    Error with query 'butterfly meme': name 'models' is not defined\n",
      "  Query 3: 'is this a butterfly'\n",
      "    Error with query 'is this a butterfly': name 'models' is not defined\n",
      "üìä Total for 'is this a pigeon': 0 meme posts\n",
      "\n",
      "‚ùå No data collected. Need to adjust search strategy.\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Enhanced collection for all 10 meme templates with corrected image extraction\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime, timezone\n",
    "import sys\n",
    "\n",
    "print(\"üéØ COMPREHENSIVE MEME DATA COLLECTION\")\n",
    "print(\"=====================================\")\n",
    "print(f\"Collecting 10 meme templates at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# research constraints\n",
    "SINCE_DATE = datetime(2024, 2, 1, tzinfo=timezone.utc)  # Step 1: only keep posts since Feb 1, 2024\n",
    "MIN_LIKES = 100  # Step 2: like threshold \n",
    "\n",
    "# All 10 meme templates\n",
    "ALL_MEME_TEMPLATES = [\n",
    "    \"change my mind\",\n",
    "    \"drake hotline bling\", \n",
    "    \"distracted boyfriend\",\n",
    "    \"two button choice\",\n",
    "    \"expanding brain\",\n",
    "    \"woman yelling at cat\",\n",
    "    \"surprised pikachu\",\n",
    "    \"mocking spongebob\",\n",
    "    \"this is fine\",\n",
    "    \"is this a pigeon\"\n",
    "]\n",
    "\n",
    "# Alternative search queries for each template\n",
    "TEMPLATE_SEARCH_VARIANTS = {\n",
    "    \"change my mind\": [\"change my mind meme\", \"changemymind\", \"#changemymind\"],\n",
    "    \"drake hotline bling\": [\"drake hotline bling meme\", \"drake meme\", \"drake reaction\"],\n",
    "    \"distracted boyfriend\": [\"distracted boyfriend meme\", \"distracted boyfriend\", \"looking at other girl\"],\n",
    "    \"two button choice\": [\"two button meme\", \"two buttons\", \"which button\"],\n",
    "    \"expanding brain\": [\"expanding brain meme\", \"brain expanding\", \"5d chess\"],\n",
    "    \"woman yelling at cat\": [\"woman yelling at cat meme\", \"yelling cat meme\", \"woman cat restaurant\"],\n",
    "    \"surprised pikachu\": [\"surprised pikachu meme\", \"pikachu surprised\", \"pikachu face\"],\n",
    "    \"mocking spongebob\": [\"mocking spongebob meme\", \"spongebob mock\", \"mockingspongebob\"],\n",
    "    \"this is fine\": [\"this is fine meme\", \"dog burning house\", \"everything is fine\"],\n",
    "    \"is this a pigeon\": [\"is this a pigeon meme\", \"butterfly meme\", \"is this a butterfly\"]\n",
    "}\n",
    "\n",
    "all_meme_data = []\n",
    "unique_post_ids = set()  # To avoid duplicates\n",
    "\n",
    "# Create processed directory if it doesn't exist\n",
    "from pathlib import Path\n",
    "PROJECT_ROOT = Path('.').resolve()\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def extract_image_url(embed_image):\n",
    "    \"\"\"Extract image URL from Bluesky ViewImage object with multiple fallbacks\"\"\"\n",
    "    try:\n",
    "        # Method 1: Check for fullsize attribute (newer structure)\n",
    "        if hasattr(embed_image, 'fullsize'):\n",
    "            return str(embed_image.fullsize)\n",
    "        \n",
    "        # Method 2: Check for image attribute with ref.link (older structure)\n",
    "        if hasattr(embed_image, 'image'):\n",
    "            if hasattr(embed_image.image, 'ref') and hasattr(embed_image.image.ref, 'link'):\n",
    "                return str(embed_image.image.ref.link)\n",
    "        \n",
    "        # Method 3: Check for thumb attribute and convert to fullsize\n",
    "        if hasattr(embed_image, 'thumb'):\n",
    "            thumb_url = str(embed_image.thumb)\n",
    "            # Try to convert thumbnail URL to fullsize\n",
    "            if 'feed_thumbnail' in thumb_url:\n",
    "                return thumb_url.replace('feed_thumbnail', 'feed_fullsize')\n",
    "            # Try other common thumbnail to fullsize conversions\n",
    "            if '/thumb/' in thumb_url:\n",
    "                return thumb_url.replace('/thumb/', '/fullsize/')\n",
    "            # Just return the thumb URL as fallback\n",
    "            return thumb_url\n",
    "        \n",
    "        # Method 4: Check for direct link in various attributes\n",
    "        for attr in ['link', 'url', 'href', 'src']:\n",
    "            if hasattr(embed_image, attr):\n",
    "                url = getattr(embed_image, attr)\n",
    "                if url and isinstance(url, str) and url.startswith('http'):\n",
    "                    return url\n",
    "        \n",
    "        # Method 5: Try to get from the object's dictionary representation\n",
    "        if hasattr(embed_image, '__dict__'):\n",
    "            for key, value in embed_image.__dict__.items():\n",
    "                if isinstance(value, str) and value.startswith('http') and any(ext in value.lower() for ext in ['.jpg', '.jpeg', '.png', '.gif', '.webp']):\n",
    "                    return value\n",
    "        \n",
    "        print(f\"  ‚ö†Ô∏è Could not extract image URL from: {dir(embed_image)[:10]}...\")\n",
    "        return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ö†Ô∏è Error extracting image URL: {e}\")\n",
    "        return None\n",
    "\n",
    "for template in ALL_MEME_TEMPLATES:\n",
    "    print(f\"\\nüîç Searching for: '{template}'\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    template_posts = []\n",
    "    search_queries = TEMPLATE_SEARCH_VARIANTS.get(template, [template, f\"{template} meme\"])\n",
    "    \n",
    "    for search_idx, search_query in enumerate(search_queries[:3]):  # Try first 3 variations\n",
    "        print(f\"  Query {search_idx+1}: '{search_query}'\")\n",
    "        \n",
    "        try:\n",
    "            # Search with pagination - get up to 100 posts per query\n",
    "            cursor = None\n",
    "            total_collected = 0\n",
    "            \n",
    "            for page in range(1, 100):  # Try up to 4 pages (25 posts per page = 100 total)\n",
    "                params = models.AppBskyFeedSearchPosts.Params(\n",
    "                    q=search_query,\n",
    "                    limit=25,  # Max per request\n",
    "                    cursor=cursor\n",
    "                )\n",
    "                \n",
    "                feed = client.app.bsky.feed.search_posts(params)\n",
    "                \n",
    "                if not feed or not hasattr(feed, 'posts') or not feed.posts:\n",
    "                    print(f\"    Page {page}: No results\")\n",
    "                    break\n",
    "                \n",
    "                page_collected = 0\n",
    "                for post in feed.posts:\n",
    "                    # Skip if we've seen this post before\n",
    "                    if post.uri in unique_post_ids:\n",
    "                        continue\n",
    "                    # üî¥ filter by created_at >= 2024-02-01\n",
    "                    created_raw = getattr(post.record, 'created_at', '')\n",
    "                    created_dt = None\n",
    "                    if created_raw:\n",
    "                        try:\n",
    "                            # Bluesky timestamps look like '2024-11-29T13:45:12.345Z'\n",
    "                            created_dt = datetime.fromisoformat(created_raw.replace('Z', '+00:00'))\n",
    "                        except Exception:\n",
    "                            created_dt = None\n",
    "                \n",
    "                    if created_dt is not None and created_dt < SINCE_DATE:\n",
    "                        # too old ‚Üí skip this post\n",
    "                        continue\n",
    "                \n",
    "                    # üî¥ get likes once & filter\n",
    "                    likes = getattr(post, 'like_count', 0)\n",
    "                    if likes < MIN_LIKES:\n",
    "                        # not popular enough according to our filter ‚Üí skip\n",
    "                        continue\n",
    "                        \n",
    "                    # Check if post has images\n",
    "                    has_image = (hasattr(post, 'embed') and \n",
    "                                hasattr(post.embed, 'images') and \n",
    "                                post.embed.images and \n",
    "                                len(post.embed.images) > 0)\n",
    "                    \n",
    "                    if has_image:\n",
    "                        try:\n",
    "                            # Extract image URL using our helper function\n",
    "                            image_url = extract_image_url(post.embed.images[0])\n",
    "                            \n",
    "                            if image_url:\n",
    "                                # Get alt text\n",
    "                                alt_text = \"\"\n",
    "                                if hasattr(post.embed.images[0], 'alt'):\n",
    "                                    alt_text = post.embed.images[0].alt\n",
    "                                \n",
    "                                # Extract data for analysis\n",
    "                                post_data = {\n",
    "                                    'platform': 'bluesky',\n",
    "                                    'meme_template': template,\n",
    "                                    'search_query': search_query,\n",
    "                                    'post_id': post.uri,\n",
    "                                    'author_handle': post.author.handle,\n",
    "                                    'author_display': getattr(post.author, 'display_name', ''),\n",
    "                                    'caption': getattr(post.record, 'text', ''),\n",
    "                                    'alt_text': alt_text,\n",
    "                                    'image_url': image_url,\n",
    "                                    'likes': getattr(post, 'like_count', 0),\n",
    "                                    'reposts': getattr(post, 'repost_count', 0),\n",
    "                                    'replies': getattr(post, 'reply_count', 0),\n",
    "                                    'created_at': getattr(post.record, 'created_at', ''),\n",
    "                                    'indexed_at': getattr(post, 'indexed_at', ''),\n",
    "                                    'post_url': f\"https://bsky.app/profile/{post.author.handle}/post/{post.uri.split('/')[-1]}\",\n",
    "                                    'collection_timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                                }\n",
    "                                \n",
    "                                template_posts.append(post_data)\n",
    "                                unique_post_ids.add(post.uri)\n",
    "                                page_collected += 1\n",
    "                                total_collected += 1\n",
    "                        \n",
    "                        except Exception as e:\n",
    "                            print(f\"    ‚ö†Ô∏è Error processing image: {e}\")\n",
    "                            # Debug: print the structure to understand what we're dealing with\n",
    "                            if page_collected == 0:  # Only print once per page for debugging\n",
    "                                print(f\"    Debug - embed.images[0] type: {type(post.embed.images[0])}\")\n",
    "                                print(f\"    Debug - embed.images[0] attributes: {[attr for attr in dir(post.embed.images[0]) if not attr.startswith('_')][:10]}\")\n",
    "                            continue\n",
    "                \n",
    "                print(f\"    Page {page}: Found {len(feed.posts)} posts, {page_collected} with images\")\n",
    "                \n",
    "                # Check for next page\n",
    "                if hasattr(feed, 'cursor') and feed.cursor:\n",
    "                    cursor = feed.cursor\n",
    "                    time.sleep(0.5)  # Rate limiting between pages\n",
    "                else:\n",
    "                    break\n",
    "            \n",
    "            print(f\"    Total from this query: {total_collected} new meme posts\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    Error with query '{search_query}': {e}\")\n",
    "            continue\n",
    "        \n",
    "        time.sleep(1)  # Rate limiting between queries\n",
    "    \n",
    "    # Add to overall collection\n",
    "    all_meme_data.extend(template_posts)\n",
    "    print(f\"üìä Total for '{template}': {len(template_posts)} meme posts\")\n",
    "    \n",
    "    # Save intermediate results after each template\n",
    "    if template_posts:\n",
    "        template_df = pd.DataFrame(template_posts)\n",
    "        template_file = PROCESSED_DIR / f\"memes_{template.replace(' ', '_')}_{datetime.now().strftime('%Y%m%d_%H%M')}.csv\"\n",
    "        template_df.to_csv(template_file, index=False, encoding='utf-8')\n",
    "        print(f\"üíæ Saved template data to: {template_file}\")\n",
    "\n",
    "# Create final DataFrame\n",
    "if all_meme_data:\n",
    "    df_comprehensive = pd.DataFrame(all_meme_data)\n",
    "\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"üéâ COLLECTION COMPLETE!\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Total meme posts collected: {len(df_comprehensive)}\")\n",
    "    print(f\"Unique posts: {len(unique_post_ids)}\")\n",
    "    \n",
    "    # Save comprehensive dataset\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "    output_file = PROCESSED_DIR / f\"all_memes_comprehensive_{timestamp}.csv\"\n",
    "    df_comprehensive.to_csv(output_file, index=False, encoding='utf-8')\n",
    "    \n",
    "    print(f\"\\nüíæ SAVED COMPREHENSIVE DATASET:\")\n",
    "    print(f\"   {output_file}\")\n",
    "    \n",
    "    # Show statistics\n",
    "    print(f\"\\nüìä BREAKDOWN BY TEMPLATE:\")\n",
    "    template_counts = df_comprehensive['meme_template'].value_counts()\n",
    "    for template, count in template_counts.items():\n",
    "        print(f\"   {template}: {count} posts\")\n",
    "    \n",
    "    print(f\"\\nüìä ENGAGEMENT STATISTICS:\")\n",
    "    print(f\"   Average likes: {df_comprehensive['likes'].mean():.1f}\")\n",
    "    print(f\"   Average reposts: {df_comprehensive['reposts'].mean():.1f}\")\n",
    "    print(f\"   Average replies: {df_comprehensive['replies'].mean():.1f}\")\n",
    "    \n",
    "    print(f\"\\nüìä SAMPLE DATA (first 3 rows):\")\n",
    "    print(df_comprehensive[['meme_template', 'author_handle', 'caption', 'likes', 'reposts']].head(3).to_string(index=False))\n",
    "    \n",
    "    # Additional analysis\n",
    "    print(f\"\\nüìä ADDITIONAL INSIGHTS:\")\n",
    "    print(f\"   Posts with images: {len(df_comprehensive)}\")\n",
    "    print(f\"   Unique authors: {df_comprehensive['author_handle'].nunique()}\")\n",
    "    print(f\"   Date range: {df_comprehensive['created_at'].min()} to {df_comprehensive['created_at'].max()}\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\n‚ùå No data collected. Need to adjust search strategy.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7c479e18-f247-4b44-8d39-908f95e4cc1c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 46 meme posts...\n",
      "\n",
      "üì• DOWNLOADING MEME IMAGES\n",
      "==========================\n",
      "  Downloading change_my_mind_jakepaulsa_0.jpg...\n",
      "    ‚úì Success: (500, 485)\n",
      "  Downloading change_my_mind_aloysiusth_1.jpg...\n",
      "    ‚úì Success: (577, 432)\n",
      "  Downloading change_my_mind_sam_robots_2.jpg...\n",
      "    ‚úì Success: (973, 720)\n",
      "  Downloading change_my_mind_9coacheswa_3.jpg...\n",
      "    ‚úì Success: (577, 432)\n",
      "  Downloading change_my_mind_misfir3_4.jpg...\n",
      "    ‚úì Success: (1206, 920)\n",
      "  Downloading change_my_mind_pdx-woman_5.jpg...\n",
      "    ‚úì Success: (577, 432)\n",
      "  Downloading change_my_mind_fritzdrybe_6.jpg...\n",
      "    ‚úì Success: (640, 480)\n",
      "  Downloading change_my_mind_lizitsthe9_7.jpg...\n",
      "    ‚úì Success: (914, 672)\n",
      "  Downloading change_my_mind_coreyogden_8.jpg...\n",
      "    ‚úì Success: (2000, 2000)\n",
      "  Downloading change_my_mind_nember_9.jpg...\n",
      "    ‚úì Success: (1061, 1200)\n",
      "  Downloading change_my_mind_trinux_mas_10.jpg...\n",
      "    ‚úì Success: (577, 432)\n",
      "  Downloading change_my_mind_freecaledo_11.jpg...\n",
      "    ‚úì Success: (577, 432)\n",
      "  Downloading change_my_mind_statesongx_12.jpg...\n",
      "    ‚úì Success: (640, 480)\n",
      "  Downloading change_my_mind_vivekgani_13.jpg...\n",
      "    ‚úì Success: (1054, 2000)\n",
      "  Downloading change_my_mind_iamozma_co_14.jpg...\n",
      "    ‚úì Success: (1111, 833)\n",
      "  Downloading change_my_mind_esquiring_15.jpg...\n",
      "    ‚úì Success: (640, 480)\n",
      "  Downloading change_my_mind_cassandraw_16.jpg...\n",
      "    ‚úì Success: (577, 432)\n",
      "  Downloading change_my_mind_byrdnick_c_17.jpg...\n",
      "    ‚úì Success: (1999, 1500)\n",
      "  Downloading change_my_mind_solarshea_18.jpg...\n",
      "    ‚úì Success: (586, 678)\n",
      "  Downloading change_my_mind_madebymart_19.jpg...\n",
      "    ‚úì Success: (1080, 384)\n",
      "  Downloading expanding_brain_julzbullar_20.jpg...\n",
      "    ‚úì Success: (500, 701)\n",
      "  Downloading expanding_brain_nurbsy_21.jpg...\n",
      "    ‚úì Success: (500, 1020)\n",
      "  Downloading expanding_brain_sonjadrimm_22.jpg...\n",
      "    ‚úì Success: (1028, 1066)\n",
      "  Downloading expanding_brain_rob_elves__23.jpg...\n",
      "    ‚úì Success: (500, 701)\n",
      "  Downloading expanding_brain_gardenofkn_24.jpg...\n",
      "    ‚úì Success: (750, 759)\n",
      "  Downloading expanding_brain_mariaziele_25.jpg...\n",
      "    ‚úì Success: (1414, 2000)\n",
      "  Downloading expanding_brain_luxalptrau_26.jpg...\n",
      "    ‚úì Success: (500, 525)\n",
      "  Downloading expanding_brain_emily_lgbt_27.jpg...\n",
      "    ‚úì Success: (1373, 1938)\n",
      "  Downloading expanding_brain_sapphixy_28.jpg...\n",
      "    ‚úì Success: (645, 960)\n",
      "  Downloading expanding_brain_wasabi-bes_29.jpg...\n",
      "    ‚úì Success: (1179, 1179)\n",
      "  Downloading expanding_brain_amagicrefl_30.jpg...\n",
      "    ‚úì Success: (513, 499)\n",
      "  Downloading expanding_brain_nora-light_31.jpg...\n",
      "    ‚úì Success: (508, 2000)\n",
      "  Downloading expanding_brain_flannelocc_32.jpg...\n",
      "    ‚úì Success: (1102, 1561)\n",
      "  Downloading expanding_brain_hardyroach_33.jpg...\n",
      "    ‚úì Success: (660, 936)\n",
      "  Downloading expanding_brain_thedevteam_34.jpg...\n",
      "    ‚úì Success: (500, 701)\n",
      "  Downloading expanding_brain_number137_35.jpg...\n",
      "    ‚úì Success: (496, 338)\n",
      "  Downloading expanding_brain_carina_del_36.jpg...\n",
      "    ‚úì Success: (482, 678)\n",
      "  Downloading expanding_brain_hueycallis_37.jpg...\n",
      "    ‚úì Success: (660, 935)\n",
      "  Downloading expanding_brain_sweets_sco_38.jpg...\n",
      "    ‚úì Success: (600, 600)\n",
      "  Downloading expanding_brain_julzbullar_39.jpg...\n",
      "    ‚úì Success: (500, 701)\n",
      "  Downloading expanding_brain_admiralmem_40.jpg...\n",
      "    ‚úì Success: (500, 701)\n",
      "  Downloading expanding_brain_nurbsy_41.jpg...\n",
      "    ‚úì Success: (500, 1020)\n",
      "  Downloading expanding_brain_sonjadrimm_42.jpg...\n",
      "    ‚úì Success: (1028, 1066)\n",
      "  Downloading expanding_brain_plosbiolog_43.jpg...\n",
      "    ‚úì Success: (1441, 814)\n",
      "  Downloading expanding_brain_plosbiolog_44.jpg...\n",
      "    ‚úì Success: (1441, 814)\n",
      "  Downloading expanding_brain_plosbiolog_45.jpg...\n",
      "    ‚úì Success: (1441, 814)\n",
      "\n",
      "üìä DOWNLOAD SUMMARY:\n",
      "  Successfully downloaded: 46\n",
      "  Errors: 0\n",
      "  Total processed: 46\n",
      "\n",
      "üíæ Saved updated data to: /Users/chenlingchen/the social web/data/processed/focused_memes_with_images_20251203_2013.csv\n",
      "\n",
      "üìà DATA STATISTICS:\n",
      "  Posts with images downloaded: 46\n",
      "  Average likes: 9.7\n",
      "  Average reposts: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Download images for all collected memes\n",
    "def download_all_meme_images(df):\n",
    "    \"\"\"Download images for comprehensive dataset\"\"\"\n",
    "    import requests\n",
    "    from PIL import Image\n",
    "    import io\n",
    "    \n",
    "    print(\"\\nüì• DOWNLOADING ALL MEME IMAGES\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Create images directory\n",
    "    IMAGES_DIR = PROJECT_ROOT / \"images\"\n",
    "    IMAGES_DIR.mkdir(exist_ok=True)\n",
    "    \n",
    "    download_count = 0\n",
    "    error_count = 0\n",
    "    df = df.copy()  # Work on a copy\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        try:\n",
    "            # Skip if no image URL\n",
    "            if not row['image_url'] or pd.isna(row['image_url']):\n",
    "                error_count += 1\n",
    "                continue\n",
    "            \n",
    "            # Create organized folder structure\n",
    "            template_folder = row['meme_template'].replace(' ', '_').lower()[:20]\n",
    "            template_dir = IMAGES_DIR / template_folder\n",
    "            template_dir.mkdir(exist_ok=True)\n",
    "            \n",
    "            # Create descriptive filename\n",
    "            author_safe = row['author_handle'].replace('.bsky.social', '').replace('.', '_')[:15]\n",
    "            date_part = row['created_at'][:10] if row['created_at'] else 'unknown'\n",
    "            filename = f\"{template_folder}_{author_safe}_{date_part}_{idx}.jpg\"\n",
    "            filepath = template_dir / filename\n",
    "            \n",
    "            # Skip if already downloaded\n",
    "            if filepath.exists():\n",
    "                df.at[idx, 'image_path'] = str(filepath)\n",
    "                download_count += 1\n",
    "                continue\n",
    "            \n",
    "            # Download image\n",
    "            print(f\"  Downloading {template_folder}/{filename}...\")\n",
    "            \n",
    "            headers = {\n",
    "                'User-Agent': 'Meme Research Project v1.0 (Academic)',\n",
    "                'Accept': 'image/*'\n",
    "            }\n",
    "            \n",
    "            response = requests.get(row['image_url'], headers=headers, timeout=20)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                # Save image\n",
    "                with open(filepath, 'wb') as f:\n",
    "                    f.write(response.content)\n",
    "                \n",
    "                # Verify it's a valid image\n",
    "                try:\n",
    "                    img = Image.open(filepath)\n",
    "                    img.verify()  # Verify it's a valid image\n",
    "                    \n",
    "                    # Store metadata\n",
    "                    df.at[idx, 'image_path'] = str(filepath)\n",
    "                    df.at[idx, 'image_width'], df.at[idx, 'image_height'] = img.size\n",
    "                    df.at[idx, 'image_format'] = img.format\n",
    "                    \n",
    "                    download_count += 1\n",
    "                    print(f\"    ‚úì {img.size} ({img.format})\")\n",
    "                    \n",
    "                except Exception as img_error:\n",
    "                    print(f\"    ‚ö†Ô∏è Invalid image: {img_error}\")\n",
    "                    filepath.unlink(missing_ok=True)\n",
    "                    error_count += 1\n",
    "            \n",
    "            else:\n",
    "                print(f\"    ‚ùå HTTP {response.status_code}\")\n",
    "                error_count += 1\n",
    "            \n",
    "            # Rate limiting\n",
    "            time.sleep(0.2)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Error row {idx}: {str(e)[:100]}\")\n",
    "            error_count += 1\n",
    "    \n",
    "    return df, download_count, error_count\n",
    "\n",
    "# Download images if we have data\n",
    "if 'df_comprehensive' in locals() and not df_comprehensive.empty:\n",
    "    print(f\"\\nüñºÔ∏è  Starting image download for {len(df_comprehensive)} posts...\")\n",
    "    \n",
    "    df_with_images, downloaded, errors = download_all_meme_images(df_comprehensive)\n",
    "    \n",
    "    print(f\"\\nüìä IMAGE DOWNLOAD SUMMARY:\")\n",
    "    print(f\"   Successfully downloaded: {downloaded}\")\n",
    "    print(f\"   Errors: {errors}\")\n",
    "    print(f\"   Success rate: {downloaded/(downloaded+errors)*100:.1f}%\")\n",
    "    \n",
    "    # Save dataset with image paths\n",
    "    final_output = PROCESSED_DIR / f\"all_memes_with_images_{timestamp}.csv\"\n",
    "    df_with_images.to_csv(final_output, index=False, encoding='utf-8')\n",
    "    \n",
    "    print(f\"\\nüíæ FINAL DATASET SAVED TO:\")\n",
    "    print(f\"   {final_output}\")\n",
    "    \n",
    "    # Create a summary report\n",
    "    summary_file = PROCESSED_DIR / f\"collection_summary_{timestamp}.txt\"\n",
    "    with open(summary_file, 'w') as f:\n",
    "        f.write(\"MEME DATA COLLECTION SUMMARY\\n\")\n",
    "        f.write(\"=\" * 40 + \"\\n\\n\")\n",
    "        f.write(f\"Collection date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(f\"Total posts collected: {len(df_with_images)}\\n\")\n",
    "        f.write(f\"Images successfully downloaded: {downloaded}\\n\")\n",
    "        f.write(f\"Download errors: {errors}\\n\\n\")\n",
    "        \n",
    "        f.write(\"BY TEMPLATE:\\n\")\n",
    "        for template, count in df_with_images['meme_template'].value_counts().items():\n",
    "            f.write(f\"  {template}: {count}\\n\")\n",
    "    \n",
    "    print(f\"\\nüìÑ Summary report saved to: {summary_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44059b5",
   "metadata": {},
   "source": [
    "## 3. Load processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b9eb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# In the final project, you will provide cleaned CSV files in data/processed.\n",
    "# Here we load them; if they do not exist yet, we show an empty DataFrame\n",
    "# so that the notebook can still run without errors.\n",
    "\n",
    "reddit_path = PROCESSED_DIR / \"reddit_memes.csv\"\n",
    "bluesky_path = PROCESSED_DIR / \"bluesky_memes.csv\"\n",
    "\n",
    "if reddit_path.exists():\n",
    "    reddit_df = pd.read_csv(reddit_path)\n",
    "else:\n",
    "    reddit_df = pd.DataFrame()\n",
    "    print(\"WARNING: reddit_memes.csv not found in data/processed. Using empty DataFrame.\")\n",
    "\n",
    "if bluesky_path.exists():\n",
    "    bluesky_df = pd.read_csv(bluesky_path)\n",
    "else:\n",
    "    bluesky_df = pd.DataFrame()\n",
    "    print(\"WARNING: bluesky_memes.csv not found in data/processed. Using empty DataFrame.\")\n",
    "\n",
    "print(\"Reddit shape:\", reddit_df.shape)\n",
    "print(\"Bluesky shape:\", bluesky_df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a9bef9",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Data cleaning and harmonization\n",
    "\n",
    "Goals:\n",
    "\n",
    "- Merge Reddit and Bluesky into a unified schema.  \n",
    "- Standardize timestamp format.  \n",
    "- Create unified fields for:\n",
    "  - `caption` (title/text)  \n",
    "  - `image_url`  \n",
    "  - `likes`, `comments`, `shares` (or platform equivalents)  \n",
    "  - `platform`  \n",
    "\n",
    "We also filter to posts that actually contain images and belong to the meme templates of interest.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606878bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example harmonization skeleton\n",
    "\n",
    "def unify_reddit(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if df.empty:\n",
    "        return df\n",
    "    out = df.copy()\n",
    "    out[\"platform\"] = \"reddit\"\n",
    "    out[\"caption\"] = out[\"title\"].fillna(\"\") + \" \" + out[\"selftext\"].fillna(\"\")\n",
    "    out[\"image_url\"] = out[\"url\"]\n",
    "    out[\"likes\"] = out[\"score\"]\n",
    "    out[\"comments\"] = out[\"num_comments\"]\n",
    "    out[\"shares\"] = np.nan  # Reddit has no explicit \"shares\"\n",
    "    out[\"created_at\"] = pd.to_datetime(out[\"created_utc\"], unit=\"s\")\n",
    "    return out[[\"platform\", \"caption\", \"image_url\", \"likes\", \"comments\", \"shares\", \"created_at\"]]\n",
    "\n",
    "def unify_bluesky(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if df.empty:\n",
    "        return df\n",
    "    out = df.copy()\n",
    "    out[\"platform\"] = \"bluesky\"\n",
    "    out[\"caption\"] = out[\"text\"]\n",
    "    out[\"image_url\"] = out.get(\"image_urls\", \"\")\n",
    "    out[\"likes\"] = out.get(\"like_count\", np.nan)\n",
    "    out[\"comments\"] = out.get(\"reply_count\", np.nan)\n",
    "    out[\"shares\"] = out.get(\"repost_count\", np.nan)\n",
    "    out[\"created_at\"] = pd.to_datetime(out[\"created_at\"])\n",
    "    return out[[\"platform\", \"caption\", \"image_url\", \"likes\", \"comments\", \"shares\", \"created_at\"]]\n",
    "\n",
    "unified_frames = []\n",
    "for df, fn in [(reddit_df, unify_reddit), (bluesky_df, unify_bluesky)]:\n",
    "    unified_frames.append(fn(df))\n",
    "\n",
    "full_df = pd.concat(unified_frames, ignore_index=True) if unified_frames else pd.DataFrame()\n",
    "print(\"Unified dataset shape:\", full_df.shape)\n",
    "full_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79b9af3",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Image download and caching\n",
    "\n",
    "We download each meme image to `images/` and keep a mapping from post ID to local file path.\n",
    "\n",
    "This enables:\n",
    "\n",
    "- reproducible CLIP embeddings  \n",
    "- offline analysis  \n",
    "- visual inspection per template cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb66c4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import hashlib\n",
    "import requests\n",
    "\n",
    "def download_image(url: str, dest_dir: Path) -> Path | None:\n",
    "    if not isinstance(url, str) or not url.startswith(\"http\"):\n",
    "        return None\n",
    "    fname = hashlib.md5(url.encode(\"utf-8\")).hexdigest() + \".jpg\"\n",
    "    fpath = dest_dir / fname\n",
    "    if fpath.exists():\n",
    "        return fpath\n",
    "    try:\n",
    "        resp = requests.get(url, timeout=10)\n",
    "        resp.raise_for_status()\n",
    "        with open(fpath, \"wb\") as f:\n",
    "            f.write(resp.content)\n",
    "        return fpath\n",
    "    except Exception as e:\n",
    "        # In the final project you may log this instead of printing\n",
    "        print(\"Failed to download\", url, \"->\", e)\n",
    "        return None\n",
    "\n",
    "if not full_df.empty and \"image_url\" in full_df.columns:\n",
    "    local_paths = []\n",
    "    for url in full_df[\"image_url\"]:\n",
    "        local_paths.append(download_image(url, IMAGES_DIR))\n",
    "    full_df[\"image_path\"] = local_paths\n",
    "else:\n",
    "    print(\"No image URLs available yet; skipping download step.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c72601a",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Image embeddings and template clustering\n",
    "\n",
    "We use a pre-trained CLIP model to obtain an embedding vector for each meme image,  \n",
    "then cluster memes into template-like groups using k-means.\n",
    "\n",
    "Later, we will label clusters with interpretable template names (e.g., \"change my mind\").\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22da8ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CLIP_ENABLED = False  # set to True when dependencies and GPU/CPU resources are available\n",
    "\n",
    "if CLIP_ENABLED and not full_df.empty and \"image_path\" in full_df.columns:\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "    embed_list = []\n",
    "    valid_idx = []\n",
    "    for idx, path in enumerate(full_df[\"image_path\"]):\n",
    "        if path and isinstance(path, str) and os.path.exists(path):\n",
    "            image = Image.open(path).convert(\"RGB\")\n",
    "            inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "            with torch.no_grad():\n",
    "                emb = model.get_image_features(**inputs)\n",
    "            embed_list.append(emb.cpu().numpy().flatten())\n",
    "            valid_idx.append(idx)\n",
    "\n",
    "    if embed_list:\n",
    "        embeds = np.vstack(embed_list)\n",
    "        n_clusters = 8  # adjust after exploration\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "        cluster_labels = kmeans.fit_predict(embeds)\n",
    "\n",
    "        full_df.loc[valid_idx, \"template_cluster\"] = cluster_labels\n",
    "    else:\n",
    "        print(\"No embeddings computed (no valid images).\")\n",
    "else:\n",
    "    print(\"CLIP embedding step disabled or no images available.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff5e619",
   "metadata": {},
   "source": [
    "### 6.1 Cluster visualization (t-SNE / example images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff45aab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: After computing template_cluster, create:\n",
    "# - t-SNE scatter plot colored by cluster\n",
    "# - montage of example images per cluster\n",
    "\n",
    "# Example placeholder for t-SNE:\n",
    "if not full_df.empty and \"template_cluster\" in full_df.columns:\n",
    "    print(\"Template clusters available. Add t-SNE visualization here.\")\n",
    "else:\n",
    "    print(\"Template clusters not yet computed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba48db2c",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Caption processing, sentiment and framing\n",
    "\n",
    "Here we:\n",
    "\n",
    "- Clean captions (lowercase, remove URLs, etc.)  \n",
    "- Compute sentiment scores (e.g., using VADER).  \n",
    "- Derive simple framing features:\n",
    "  - presence of question marks / exclamations  \n",
    "  - first-person vs. third-person pronouns  \n",
    "  - political vs. non-political keywords  \n",
    "- (Optional) Use topic modeling or more advanced framing classifiers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b3d30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "\n",
    "def basic_clean(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "if not full_df.empty:\n",
    "    full_df[\"caption_clean\"] = full_df[\"caption\"].apply(basic_clean)\n",
    "else:\n",
    "    print(\"Unified dataset empty; skipping caption cleaning.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5474139",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sentiment analysis with VADER (if available)\n",
    "try:\n",
    "    nltk.data.find(\"sentiment/vader_lexicon.zip\")\n",
    "except LookupError:\n",
    "    try:\n",
    "        nltk.download(\"vader_lexicon\")\n",
    "    except Exception as e:\n",
    "        print(\"Could not download VADER lexicon:\", e)\n",
    "\n",
    "if not full_df.empty:\n",
    "    try:\n",
    "        sia = SentimentIntensityAnalyzer()\n",
    "        sentiment_scores = full_df[\"caption_clean\"].apply(lambda t: sia.polarity_scores(t)[\"compound\"])\n",
    "        full_df[\"sentiment\"] = sentiment_scores\n",
    "    except Exception as e:\n",
    "        print(\"Sentiment analysis failed:\", e)\n",
    "else:\n",
    "    print(\"Unified dataset empty; skipping sentiment computation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae640692",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Simple framing features (placeholder)\n",
    "if not full_df.empty:\n",
    "    full_df[\"has_question\"] = full_df[\"caption_clean\"].str.contains(\"\\?\")\n",
    "    full_df[\"has_exclaim\"] = full_df[\"caption_clean\"].str.contains(\"!\")\n",
    "    full_df[\"length_tokens\"] = full_df[\"caption_clean\"].str.split().apply(len)\n",
    "\n",
    "    # Example: detect potentially political captions using keyword list\n",
    "    political_keywords = [\"election\", \"president\", \"government\", \"policy\", \"vote\"]\n",
    "    pattern = \"|\".join(political_keywords)\n",
    "    full_df[\"is_political\"] = full_df[\"caption_clean\"].str.contains(pattern)\n",
    "else:\n",
    "    print(\"Unified dataset empty; skipping framing features.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f081e405",
   "metadata": {},
   "source": [
    "\n",
    "## 8. Virality metrics\n",
    "\n",
    "We operationalize **virality** as a function of:\n",
    "\n",
    "- Likes / upvotes  \n",
    "- Comments / replies  \n",
    "- Shares / reposts  \n",
    "- (Optional) Time-normalized variants (e.g., score per hour)\n",
    "\n",
    "We then examine how virality varies across:\n",
    "\n",
    "- meme templates (clusters)  \n",
    "- caption sentiment and framing features  \n",
    "- platforms (Reddit vs Bluesky)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2021df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not full_df.empty:\n",
    "    # Example composite virality score (you may refine this):\n",
    "    def compute_virality(row):\n",
    "        likes = row.get(\"likes\", 0) or 0\n",
    "        comments = row.get(\"comments\", 0) or 0\n",
    "        shares = row.get(\"shares\", 0) or 0\n",
    "        return likes + 2*comments + 3*shares\n",
    "\n",
    "    full_df[\"virality_raw\"] = full_df.apply(compute_virality, axis=1)\n",
    "\n",
    "    # Log-transform to reduce skew\n",
    "    full_df[\"virality_log1p\"] = np.log1p(full_df[\"virality_raw\"])\n",
    "else:\n",
    "    print(\"Unified dataset empty; skipping virality computation.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87d7d67",
   "metadata": {},
   "source": [
    "\n",
    "## 9. Modeling: how template and caption framing influence virality\n",
    "\n",
    "We fit simple models to understand joint effects of:\n",
    "\n",
    "- Template cluster  \n",
    "- Sentiment  \n",
    "- Framing features  \n",
    "- Platform  \n",
    "\n",
    "on virality.\n",
    "\n",
    "Possible approaches:\n",
    "\n",
    "- Linear regression / regularized regression  \n",
    "- Mixed-effects models (platform as random effect)  \n",
    "- Tree-based models (e.g., RandomForestRegressor)  \n",
    "\n",
    "Below we show a basic regression skeleton.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908ad6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "except ImportError:\n",
    "    print(\"scikit-learn not installed; install it to run the regression.\")\n",
    "else:\n",
    "    if not full_df.empty and \"virality_log1p\" in full_df.columns:\n",
    "        model_df = full_df.dropna(subset=[\"virality_log1p\"]).copy()\n",
    "\n",
    "        # Create simple numeric features\n",
    "        model_df[\"platform_is_reddit\"] = (model_df[\"platform\"] == \"reddit\").astype(int)\n",
    "        model_df[\"template_cluster\"] = model_df.get(\"template_cluster\", -1).fillna(-1).astype(int)\n",
    "\n",
    "        feature_cols = [\n",
    "            \"platform_is_reddit\",\n",
    "            \"template_cluster\",\n",
    "            \"sentiment\",\n",
    "            \"has_question\",\n",
    "            \"has_exclaim\",\n",
    "            \"is_political\",\n",
    "            \"length_tokens\",\n",
    "        ]\n",
    "\n",
    "        # Convert booleans to ints\n",
    "        for col in [\"has_question\", \"has_exclaim\", \"is_political\"]:\n",
    "            if col in model_df.columns:\n",
    "                model_df[col] = model_df[col].astype(int)\n",
    "\n",
    "        X = model_df[feature_cols].fillna(0)\n",
    "        y = model_df[\"virality_log1p\"]\n",
    "\n",
    "        if len(model_df) > 10:\n",
    "            reg = LinearRegression()\n",
    "            reg.fit(X, y)\n",
    "            print(\"Fitted LinearRegression with coefficients:\")\n",
    "            for col, coef in zip(feature_cols, reg.coef_):\n",
    "                print(f\"  {col}: {coef:.3f}\")\n",
    "        else:\n",
    "            print(\"Not enough data points to fit a regression model.\")\n",
    "    else:\n",
    "        print(\"No virality data available for modeling.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b138a8",
   "metadata": {},
   "source": [
    "\n",
    "## 10. Main figures and tables\n",
    "\n",
    "This section should reproduce all main figures, charts and tables used in the paper, such as:\n",
    "\n",
    "1. Distribution of virality by platform  \n",
    "2. Sentiment vs. virality  \n",
    "3. Template cluster vs. virality  \n",
    "4. Interaction plots (e.g., sentiment‚Äìvirality by platform)  \n",
    "5. Example memes per template cluster (qualitative case studies)\n",
    "\n",
    "Below we include placeholders for you to fill with the actual visualizations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d657b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 10.1 Virality distributions by platform\n",
    "if not full_df.empty and \"virality_log1p\" in full_df.columns:\n",
    "    full_df.boxplot(column=\"virality_log1p\", by=\"platform\")\n",
    "    plt.title(\"Virality (log1p) by platform\")\n",
    "    plt.suptitle(\"\")\n",
    "    plt.xlabel(\"Platform\")\n",
    "    plt.ylabel(\"log1p(virality)\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No virality data available for plotting.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd27bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 10.2 Sentiment vs. virality scatter (placeholder)\n",
    "if not full_df.empty and \"sentiment\" in full_df.columns and \"virality_log1p\" in full_df.columns:\n",
    "    plt.scatter(full_df[\"sentiment\"], full_df[\"virality_log1p\"], alpha=0.3)\n",
    "    plt.xlabel(\"Sentiment (compound)\")\n",
    "    plt.ylabel(\"log1p(virality)\")\n",
    "    plt.title(\"Sentiment vs. virality\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Sentiment or virality not available for plotting.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a09dba7",
   "metadata": {},
   "source": [
    "\n",
    "## 11. Qualitative case studies\n",
    "\n",
    "Here you can:\n",
    "\n",
    "- Show a small set of example memes from key template clusters  \n",
    "- Compare how the **same template** is used on Reddit vs. Bluesky  \n",
    "- Discuss differences in caption framing and user reaction  \n",
    "\n",
    "You can use `IPython.display.display(Image(...))` to show example images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36817b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Implement helper to display example memes per cluster / platform.\n",
    "# from IPython.display import display\n",
    "# display(Image(filename=some_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3e10fa",
   "metadata": {},
   "source": [
    "\n",
    "## 12. Limitations and ethical considerations\n",
    "\n",
    "In this markdown cell, discuss:\n",
    "\n",
    "- Sampling biases (which subreddits / hashtags you used)  \n",
    "- API limitations (rate limits, deleted posts, etc.)  \n",
    "- Representativeness of Reddit and Bluesky users  \n",
    "- Potential ethical issues (e.g., sensitive content, privacy)  \n",
    "- Constraints of using automated sentiment / framing models on memes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd648345",
   "metadata": {},
   "source": [
    "\n",
    "## 13. Summary and link to paper\n",
    "\n",
    "Summarize the main findings that this notebook reproduces, in a few bullet points:\n",
    "\n",
    "- How visual templates relate to virality  \n",
    "- How caption framing and sentiment relate to virality  \n",
    "- Key differences between Reddit and Bluesky  \n",
    "\n",
    "Reference the corresponding sections / figures in your paper so graders can easily cross-check.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
