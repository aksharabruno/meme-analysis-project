{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "007c5775",
   "metadata": {},
   "source": [
    "\n",
    "# Meme Virality Across Platforms: Notebook\n",
    "**Group 23: Hanan F. Anderloni , Lingchen Chen, Feihong Tian, Akshara Bruno** \n",
    "\n",
    "**Research question:**  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53bad7d",
   "metadata": {},
   "source": [
    "## 1. Imports and global configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "baa77f08-58b0-4064-9c27-e457a6028ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy<2.0\n",
      "  Downloading numpy-1.26.4-cp39-cp39-macosx_10_9_x86_64.whl (20.6 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m20.6/20.6 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.1\n",
      "    Uninstalling numpy-2.0.1:\n",
      "      Successfully uninstalled numpy-2.0.1\n",
      "Successfully installed numpy-1.26.4\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install \"numpy<2.0\" --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9934df98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/chenlingchen/the social web\n"
     ]
    }
   ],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Data & analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Machine learning / embeddings / clustering\n",
    "try:\n",
    "    from sklearn.cluster import KMeans\n",
    "    from sklearn.manifold import TSNE\n",
    "except ImportError:\n",
    "    print(\"scikit-learn is not installed. Install it to enable clustering and t-SNE visualizations.\")\n",
    "\n",
    "# Deep learning / CLIP (for image embeddings)\n",
    "try:\n",
    "    import torch\n",
    "    from PIL import Image\n",
    "    from transformers import CLIPProcessor, CLIPModel\n",
    "except ImportError:\n",
    "    print(\"torch / transformers / pillow not installed. Install them to enable image embeddings.\")\n",
    "\n",
    "# NLP\n",
    "try:\n",
    "    import nltk\n",
    "    from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "except ImportError:\n",
    "    print(\"nltk not installed. Install it to enable VADER sentiment analysis.\")\n",
    "\n",
    "# Global paths\n",
    "PROJECT_ROOT = Path('.').resolve()\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "RAW_DIR = DATA_DIR / \"raw\"\n",
    "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
    "IMAGES_DIR = PROJECT_ROOT / \"images\"\n",
    "\n",
    "for p in [DATA_DIR, RAW_DIR, PROCESSED_DIR, IMAGES_DIR]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Project root:\", PROJECT_ROOT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9875f57",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Data collection (overview)\n",
    "\n",
    "We collect meme posts from:\n",
    "\n",
    "- **Reddit** â€“ using the Reddit API (e.g. PRAW or Pushshift-like interface).  \n",
    "- **Bluesky** â€“ using the ATProto API.\n",
    "\n",
    "For each platform we aim to obtain:\n",
    "\n",
    "- Post ID, author, timestamp  \n",
    "- Caption / title text  \n",
    "- Image URL(s)  \n",
    "- Engagement statistics (upvotes / likes, comments / replies, reposts)  \n",
    "- Platform indicator\n",
    "\n",
    "In practice, the actual scraping code may be placed in a separate script to avoid  \n",
    "API credential issues inside the notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ee2f3c",
   "metadata": {},
   "source": [
    "### 2.1 Reddit scraping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105f5d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "\n",
    "# --- FILTER CONFIGURATION ---\n",
    "# Start from 2005-02-01 00:00:00 UTC\n",
    "start_dt = datetime.datetime(2005, 1, 1, tzinfo=datetime.timezone.utc)\n",
    "TIMESTAMP_START = start_dt.timestamp()\n",
    "\n",
    "RAW_DIR = Path(\"data/raw\")\n",
    "reddit_path = RAW_DIR / \"reddit_memes_raw_2005_01_onward_variants.csv\"\n",
    "\n",
    "REDDIT_ENABLED = True\n",
    "\n",
    "if REDDIT_ENABLED:\n",
    "    reddit = praw.Reddit(\n",
    "        client_id=\"Dhu5BDkroWpRBQ\",\n",
    "        client_secret=\"ojQeyIZ793E7I6fRFILOiLdJzdGNcg\",\n",
    "        user_agent=\"meme-research-notebook\"\n",
    "    )\n",
    "\n",
    "    template_queries = {\n",
    "    # Original 10 with MORE variants\n",
    "    \"change my mind\": [\n",
    "        '\"change my mind\"',\n",
    "        'change-my-mind',\n",
    "        'crowder table',\n",
    "        '\"change my mind meme\"',\n",
    "        'steven crowder table',\n",
    "        'cmm table',\n",
    "    ],\n",
    "    \"expanding brain\": [\n",
    "        '\"expanding brain\"',\n",
    "        'galaxy brain',\n",
    "        'expanding brain meme',\n",
    "        '\"brain expansion\"',\n",
    "        'increasing brain size',\n",
    "        'evolving brain',\n",
    "    ],\n",
    "    \"drake hotline bling\": [\n",
    "        '\"drake hotline bling\"',\n",
    "        '\"drake meme\"',\n",
    "        '\"drakeposting\"',\n",
    "        'drake approves',\n",
    "        'drake hotline',\n",
    "        '\"drake format\"',\n",
    "    ],\n",
    "    \"distracted boyfriend\": [\n",
    "        '\"distracted boyfriend\"',\n",
    "        '\"distracted bf\"',\n",
    "        '\"man looking at other girl\"',\n",
    "        '\"guy looking back\"',\n",
    "        'distracted boyfriend meme',\n",
    "        '\"cheating boyfriend meme\"',\n",
    "    ],\n",
    "    \"two-button choice\": [\n",
    "        '\"two buttons\"',\n",
    "        '\"red blue button\"',\n",
    "        '\"two button dilemma\"',\n",
    "        'red or blue pill',\n",
    "        '\"button choice meme\"',\n",
    "        '\"hard choice buttons\"',\n",
    "    ],\n",
    "    \"woman yelling at cat\": [\n",
    "        '\"woman yelling at cat\"',\n",
    "        '\"yelling at cat\"',\n",
    "        '\"smudge the cat\"',\n",
    "        '\"woman screaming cat\"',\n",
    "        'cat smug meme',\n",
    "        '\"angry woman cat\"',\n",
    "    ],\n",
    "    \"surprised pikachu\": [\n",
    "        '\"surprised pikachu\"',\n",
    "        '\"pikachu shocked\"',\n",
    "        '\"pikachu surprise\"',\n",
    "        '\"pikachu face\"',\n",
    "        'shocked pikachu meme',\n",
    "        '\"pikachu didnt know\"',\n",
    "    ],\n",
    "    \"mocking spongebob\": [\n",
    "        '\"mocking spongebob\"',\n",
    "        '\"spongebob mocking\"',\n",
    "        '\"spongemock\"',\n",
    "        '\"mocking sponge\"',\n",
    "        'alternating case spongebob',\n",
    "        '\"spongebob mock\"',\n",
    "    ],\n",
    "    \"this is fine\": [\n",
    "        '\"this is fine\"',\n",
    "        '\"this is fine dog\"',\n",
    "        '\"burning room dog\"',\n",
    "        '\"kc green dog\"',\n",
    "        'dog in fire meme',\n",
    "        '\"everything fine dog\"',\n",
    "    ],\n",
    "    \"is this a pigeon\": [\n",
    "        '\"is this a pigeon\"',\n",
    "        '\"pigeon guy\"',\n",
    "        '\"butterfly meme guy\"',\n",
    "        '\"is this a mood\"',\n",
    "        'renamon pigeon',\n",
    "        '\"pigeon confusion\"',\n",
    "    ],\n",
    "    }\n",
    "\n",
    "    TARGET_PER_TEMPLATE = 100\n",
    "    MAX_RESULTS_PER_QUERY = 300  # per individual variant query\n",
    "    current_time_iso = datetime.datetime.utcnow().isoformat()\n",
    "\n",
    "    all_posts = []\n",
    "    seen_ids = set()\n",
    "\n",
    "    print(\"ðŸš€ Starting Reddit data collection with query variants...\")\n",
    "    print(f\"   Filtering for posts from {start_dt.isoformat()}\")\n",
    "    print(\"   No minimum score filter\")\n",
    "    print(f\"   Target per canonical template: {TARGET_PER_TEMPLATE}\")\n",
    "\n",
    "    for template, queries in template_queries.items():\n",
    "        print(f\"\\n=== Template: {template!r} ===\")\n",
    "        collected_for_template = 0\n",
    "\n",
    "        for q in queries:\n",
    "            if collected_for_template >= TARGET_PER_TEMPLATE:\n",
    "                break\n",
    "\n",
    "            print(f\"  â†’ Searching for variant query: {q!r}\")\n",
    "            for submission in reddit.subreddit(\"memes+funny+dankmemes\").search(\n",
    "                q,\n",
    "                sort=\"new\",\n",
    "                time_filter=\"all\",  # allow older than 1 year but filter in Python\n",
    "                limit=MAX_RESULTS_PER_QUERY,\n",
    "            ):\n",
    "                if submission.id in seen_ids:\n",
    "                    continue\n",
    "\n",
    "                # Only enforce date now (no likes/score filter)\n",
    "                if submission.created_utc < TIMESTAMP_START:\n",
    "                    continue\n",
    "\n",
    "                author_name = \"[deleted]\"\n",
    "                if submission.author is not None:\n",
    "                    author_name = submission.author.name\n",
    "\n",
    "                all_posts.append({\n",
    "                    \"platform\": \"reddit\",\n",
    "                    \"meme_template\": template,  # canonical label\n",
    "                    \"search_query\": q,          # actual query string used\n",
    "                    \"post_id\": submission.id,\n",
    "                    \"author_handle\": author_name,\n",
    "                    \"author_display\": author_name,\n",
    "                    \"caption\": submission.title,\n",
    "                    \"alt_text\": \"\",\n",
    "                    \"image_url\": submission.url,\n",
    "                    \"likes\": submission.score,\n",
    "                    \"reposts\": None,\n",
    "                    \"replies\": submission.num_comments,\n",
    "                    \"created_at\": datetime.datetime.fromtimestamp(\n",
    "                        submission.created_utc, tz=datetime.timezone.utc\n",
    "                    ).isoformat(),\n",
    "                    \"indexed_at\": current_time_iso,\n",
    "                    \"post_url\": f\"https://www.reddit.com{submission.permalink}\",\n",
    "                    \"image_path\": None,\n",
    "                    \"image_width\": None,\n",
    "                    \"image_height\": None\n",
    "                })\n",
    "\n",
    "                seen_ids.add(submission.id)\n",
    "                collected_for_template += 1\n",
    "\n",
    "                if collected_for_template >= TARGET_PER_TEMPLATE:\n",
    "                    print(f\"   âœ… Reached {TARGET_PER_TEMPLATE} posts for {template!r}\")\n",
    "                    break\n",
    "\n",
    "        print(f\"   Collected {collected_for_template} posts for {template!r} after filtering.\")\n",
    "\n",
    "    reddit_df = pd.DataFrame(all_posts)\n",
    "    RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    reddit_df.to_csv(reddit_path, index=False)\n",
    "    print(f\"\\nâœ… Saved raw Reddit data to {reddit_path}.\")\n",
    "    print(f\"Total posts collected after filtering: {len(reddit_df)}\")\n",
    "\n",
    "else:\n",
    "    print(\"Reddit scraping disabled. Set REDDIT_ENABLED=True after configuring credentials.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b950828d",
   "metadata": {},
   "source": [
    "### Maybe delete if it's not working? 2.1 Reddit scraping skeleton (to run once, offline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92d61bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reddit scraping disabled. Set REDDIT_ENABLED=True after configuring credentials.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# NOTE: This is a placeholder illustrating how Reddit data could be collected.\n",
    "# In the final version you may move this into a separate script and run it once,\n",
    "# then store the results as CSV(s) in data/raw or data/processed.\n",
    "\n",
    "REDDIT_ENABLED = False  # set to True when you configure credentials\n",
    "\n",
    "if REDDIT_ENABLED:\n",
    "    import praw  # pip install praw\n",
    "\n",
    "    reddit = praw.Reddit(\n",
    "        client_id=\"YOUR_CLIENT_ID\",\n",
    "        client_secret=\"YOUR_CLIENT_SECRET\",\n",
    "        user_agent=\"meme-research-notebook\"\n",
    "    )\n",
    "\n",
    "    meme_templates = [\"change my mind\", \"expanding brain\"]  # extend as needed\n",
    "    posts = []\n",
    "\n",
    "    for query in meme_templates:\n",
    "        for submission in reddit.subreddit(\"memes+funny+dankmemes\").search(query, limit=500):\n",
    "            posts.append({\n",
    "                \"platform\": \"reddit\",\n",
    "                \"template_query\": query,\n",
    "                \"id\": submission.id,\n",
    "                \"subreddit\": submission.subreddit.display_name,\n",
    "                \"title\": submission.title,\n",
    "                \"selftext\": submission.selftext,\n",
    "                \"url\": submission.url,\n",
    "                \"created_utc\": submission.created_utc,\n",
    "                \"score\": submission.score,\n",
    "                \"num_comments\": submission.num_comments\n",
    "            })\n",
    "\n",
    "    reddit_df = pd.DataFrame(posts)\n",
    "    RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    reddit_path = RAW_DIR / \"reddit_memes_raw.csv\"\n",
    "    reddit_df.to_csv(reddit_path, index=False)\n",
    "    print(\"Saved raw Reddit data to\", reddit_path)\n",
    "else:\n",
    "    print(\"Reddit scraping disabled. Set REDDIT_ENABLED=True after configuring credentials.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e72eff",
   "metadata": {},
   "source": [
    "### 2.2 Bluesky scraping skeleton (to run once, offline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa4ae7f-4957-488f-93cf-6903900dde46",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install atproto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35536dd2-9657-4cc1-9d3b-304dd81ff1bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to login...\n",
      "\n",
      "Trying to login with: feih16.bsky.social\n",
      "âœ… SUCCESS with feih16.bsky.social\n",
      "   Logged in as: feih16.bsky.social\n",
      "\n",
      "ðŸ“Š Found 3 posts for 'change my mind'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# explore post structure\\nfor i, post in enumerate(feed.posts):\\n    print(f\"\\n{\\'=\\'*60}\")\\n    print(f\"POST {i+1}\")\\n    print(\\'=\\'*60)\\n    \\n    # Basic info\\n    print(f\"URI (unique ID): {post.uri}\")\\n    print(f\"Author: {post.author.handle}\")\\n    \\n    # Text content\\n    if hasattr(post, \\'record\\'):\\n        text = post.record.text[:150]\\n        print(f\"Text: {text}...\" if len(post.record.text) > 150 else f\"Text: {post.record.text}\")\\n        print(f\"Created at: {post.record.created_at}\")\\n    \\n    # Check for images\\n    has_images = False\\n    if hasattr(post, \\'embed\\') and hasattr(post.embed, \\'images\\'):\\n        image_count = len(post.embed.images)\\n        print(f\"Has images: YES ({image_count} images)\")\\n        for img_idx, img in enumerate(post.embed.images[:2]):  # Show first 2\\n            if hasattr(img, \\'image\\'):\\n                print(f\"  Image {img_idx+1}: {img.image}\")\\n        has_images = True\\n    else:\\n        print(\"Has images: NO\")\\n    \\n    # Engagement metrics\\n    print(f\"Likes: {getattr(post, \\'like_count\\', 0)}\")\\n    print(f\"Reposts: {getattr(post, \\'repost_count\\', 0)}\")\\n    print(f\"Replies: {getattr(post, \\'reply_count\\', 0)}\")\\n    \\n    # Quick summary for our project\\n    print(f\"\\nâœ… FOR OUR PROJECT: This post has {\\'images\\' if has_images else \\'NO images\\'} and text for analysis\")\\n    '"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step 1: log in\n",
    "print(\"Trying to login...\")\n",
    "\n",
    "from atproto import Client\n",
    "\n",
    "client = Client()\n",
    "\n",
    "# Try both options\n",
    "identifiers = [\n",
    "    \"feih16.bsky.social\",  # handle without @\n",
    "    \"tianfh616@gmail.com\",  # your email if handle doesn't work\n",
    "    \"@feih16.bsky.social\",  # handle with @ (less likely to work)\n",
    "]\n",
    "\n",
    "for identifier in identifiers:\n",
    "    print(f\"\\nTrying to login with: {identifier}\")\n",
    "    try:\n",
    "        session = client.login(identifier, \"3eqo-vpdc-u7u5-q47d\")\n",
    "        print(f\"âœ… SUCCESS with {identifier}\")\n",
    "        print(f\"   Logged in as: {session.handle}\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed: {e}\")\n",
    "\n",
    "# Now that we're logged in, let's understand post structure\n",
    "from atproto import Client, models\n",
    "\n",
    "# We already have a logged-in client \n",
    "# Let's test the search with the authenticated client\n",
    "params = models.AppBskyFeedSearchPosts.Params(\n",
    "    q=\"change my mind\",\n",
    "    limit=3  # Get 3 posts to see variety\n",
    ")\n",
    "\n",
    "feed = client.app.bsky.feed.search_posts(params)\n",
    "\n",
    "print(f\"\\nðŸ“Š Found {len(feed.posts)} posts for 'change my mind'\")\n",
    "\n",
    "'''\n",
    "# explore post structure\n",
    "for i, post in enumerate(feed.posts):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"POST {i+1}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    # Basic info\n",
    "    print(f\"URI (unique ID): {post.uri}\")\n",
    "    print(f\"Author: {post.author.handle}\")\n",
    "    \n",
    "    # Text content\n",
    "    if hasattr(post, 'record'):\n",
    "        text = post.record.text[:150]\n",
    "        print(f\"Text: {text}...\" if len(post.record.text) > 150 else f\"Text: {post.record.text}\")\n",
    "        print(f\"Created at: {post.record.created_at}\")\n",
    "    \n",
    "    # Check for images\n",
    "    has_images = False\n",
    "    if hasattr(post, 'embed') and hasattr(post.embed, 'images'):\n",
    "        image_count = len(post.embed.images)\n",
    "        print(f\"Has images: YES ({image_count} images)\")\n",
    "        for img_idx, img in enumerate(post.embed.images[:2]):  # Show first 2\n",
    "            if hasattr(img, 'image'):\n",
    "                print(f\"  Image {img_idx+1}: {img.image}\")\n",
    "        has_images = True\n",
    "    else:\n",
    "        print(\"Has images: NO\")\n",
    "    \n",
    "    # Engagement metrics\n",
    "    print(f\"Likes: {getattr(post, 'like_count', 0)}\")\n",
    "    print(f\"Reposts: {getattr(post, 'repost_count', 0)}\")\n",
    "    print(f\"Replies: {getattr(post, 'reply_count', 0)}\")\n",
    "    \n",
    "    # Quick summary for our project\n",
    "    print(f\"\\nâœ… FOR OUR PROJECT: This post has {'images' if has_images else 'NO images'} and text for analysis\")\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8acfa7-f69b-4bf6-b2b6-827a4a2192ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 2: Enhanced collection for all 10 meme templates with corrected image extraction\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime, timezone, timedelta\n",
    "import sys\n",
    "\n",
    "print(\"ðŸŽ¯ COMPREHENSIVE MEME DATA COLLECTION\")\n",
    "print(\"=====================================\")\n",
    "print(f\"Collecting 10 meme templates at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# research constraints\n",
    "SINCE_DATE = datetime(2024, 2, 1, tzinfo=timezone.utc)  # Step 1: only keep posts since Feb 1, 2024\n",
    "START_DATE = datetime(2024, 2, 1) \n",
    "END_DATE = datetime.now()         \n",
    "WINDOW_DAYS = 7                    \n",
    "MIN_LIKES = 100  # Step 2: like threshold \n",
    "\n",
    "# All 10 meme templates\n",
    "ALL_MEME_TEMPLATES = [\n",
    "    \"change my mind\",\n",
    "    \"drake hotline bling\", \n",
    "    \"distracted boyfriend\",\n",
    "    \"two button choice\",\n",
    "    \"expanding brain\",\n",
    "    \"woman yelling at cat\",\n",
    "    \"surprised pikachu\",\n",
    "    \"mocking spongebob\",\n",
    "    \"this is fine\",\n",
    "    \"is this a pigeon\"\n",
    "]\n",
    "\n",
    "# Alternative search queries for each template\n",
    "TEMPLATE_SEARCH_VARIANTS = {\n",
    "    \"change my mind\": [\"change my mind meme\", \"changemymind\", \"#changemymind\"],\n",
    "    \"drake hotline bling\": [\"drake hotline bling meme\", \"drake meme\", \"drake reaction\"],\n",
    "    \"distracted boyfriend\": [\"distracted boyfriend meme\", \"distracted boyfriend\", \"looking at other girl\"],\n",
    "    \"two button choice\": [\"two button meme\", \"two buttons\", \"which button\"],\n",
    "    \"expanding brain\": [\"expanding brain meme\", \"brain expanding\", \"5d chess\"],\n",
    "    \"woman yelling at cat\": [\"woman yelling at cat meme\", \"yelling cat meme\", \"woman cat restaurant\"],\n",
    "    \"surprised pikachu\": [\"surprised pikachu meme\", \"pikachu surprised\", \"pikachu face\"],\n",
    "    \"mocking spongebob\": [\"mocking spongebob meme\", \"spongebob mock\", \"mockingspongebob\"],\n",
    "    \"this is fine\": [\"this is fine meme\", \"dog burning house\", \"everything is fine\"],\n",
    "    \"is this a pigeon\": [\"is this a pigeon meme\", \"butterfly meme\", \"is this a butterfly\"]\n",
    "}\n",
    "\n",
    "all_meme_data = []\n",
    "unique_post_ids = set()  # To avoid duplicates\n",
    "\n",
    "# Create processed directory if it doesn't exist\n",
    "from pathlib import Path\n",
    "PROJECT_ROOT = Path('.').resolve()\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def extract_image_url(embed_image):\n",
    "    \"\"\"Extract image URL from Bluesky ViewImage object with multiple fallbacks\"\"\"\n",
    "    try:\n",
    "        # Method 1: Check for fullsize attribute (newer structure)\n",
    "        if hasattr(embed_image, 'fullsize'):\n",
    "            return str(embed_image.fullsize)\n",
    "        \n",
    "        # Method 2: Check for image attribute with ref.link (older structure)\n",
    "        if hasattr(embed_image, 'image'):\n",
    "            if hasattr(embed_image.image, 'ref') and hasattr(embed_image.image.ref, 'link'):\n",
    "                return str(embed_image.image.ref.link)\n",
    "        \n",
    "        # Method 3: Check for thumb attribute and convert to fullsize\n",
    "        if hasattr(embed_image, 'thumb'):\n",
    "            thumb_url = str(embed_image.thumb)\n",
    "            # Try to convert thumbnail URL to fullsize\n",
    "            if 'feed_thumbnail' in thumb_url:\n",
    "                return thumb_url.replace('feed_thumbnail', 'feed_fullsize')\n",
    "            # Try other common thumbnail to fullsize conversions\n",
    "            if '/thumb/' in thumb_url:\n",
    "                return thumb_url.replace('/thumb/', '/fullsize/')\n",
    "            # Just return the thumb URL as fallback\n",
    "            return thumb_url\n",
    "        \n",
    "        # Method 4: Check for direct link in various attributes\n",
    "        for attr in ['link', 'url', 'href', 'src']:\n",
    "            if hasattr(embed_image, attr):\n",
    "                url = getattr(embed_image, attr)\n",
    "                if url and isinstance(url, str) and url.startswith('http'):\n",
    "                    return url\n",
    "        \n",
    "        # Method 5: Try to get from the object's dictionary representation\n",
    "        if hasattr(embed_image, '__dict__'):\n",
    "            for key, value in embed_image.__dict__.items():\n",
    "                if isinstance(value, str) and value.startswith('http') and any(ext in value.lower() for ext in ['.jpg', '.jpeg', '.png', '.gif', '.webp']):\n",
    "                    return value\n",
    "        \n",
    "        print(f\"Could not extract image URL from: {dir(embed_image)[:10]}...\")\n",
    "        return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting image URL: {e}\")\n",
    "        return None\n",
    "\n",
    "for template in ALL_MEME_TEMPLATES:\n",
    "    print(f\"\\nSearching for: '{template}'\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    template_posts = []\n",
    "    search_queries = TEMPLATE_SEARCH_VARIANTS.get(template, [template, f\"{template} meme\"])\n",
    "    \n",
    "    for search_idx, search_query in enumerate(search_queries[:3]):  # Try first 3 variations\n",
    "        print(f\"  Query {search_idx+1}: '{search_query}'\")\n",
    "        \n",
    "        try:\n",
    "            total_collected_query = 0\n",
    "            \n",
    "            # time slice loop\n",
    "            window_start = START_DATE\n",
    "            \n",
    "            while window_start < END_DATE:\n",
    "                # calculate current window end time\n",
    "                window_end = window_start + timedelta(days=WINDOW_DAYS)\n",
    "                if window_end > END_DATE:\n",
    "                    window_end = END_DATE\n",
    "                \n",
    "                # transform API needed format(for example: 2024-02-01T00:00:00Z)\n",
    "                since_str = window_start.isoformat() + \"Z\"\n",
    "                until_str = window_end.isoformat() + \"Z\"\n",
    "                \n",
    "                print(f\"Processing Window: {window_start.date()} -> {window_end.date()}\")\n",
    "                \n",
    "                cursor = None\n",
    "                page_num = 1  # calculate page number for logging\n",
    "                \n",
    "                # --- pagination loop  ---\n",
    "                while True:\n",
    "                    try:\n",
    "                        params = models.AppBskyFeedSearchPosts.Params(\n",
    "                            q=search_query,\n",
    "                            limit=25,       # Max per request (Bluesky limit)\n",
    "                            cursor=cursor,\n",
    "                            since=since_str, \n",
    "                            until=until_str, \n",
    "                            sort=\"latest\"\n",
    "                        )\n",
    "                        \n",
    "                        feed = client.app.bsky.feed.search_posts(params)\n",
    "                        \n",
    "                        if not feed or not hasattr(feed, 'posts') or not feed.posts:\n",
    "                            break\n",
    "                        \n",
    "                        page_collected = 0\n",
    "                        for post in feed.posts:\n",
    "                            # Skip if we've seen this post before\n",
    "                            if post.uri in unique_post_ids:\n",
    "                                continue\n",
    "                            \n",
    "                            # --- date doublecheck (optional) ---\n",
    "                            created_raw = getattr(post.record, 'created_at', '')\n",
    "                            \n",
    "                            #  get likes once & filter\n",
    "                            likes = getattr(post, 'like_count', 0)\n",
    "                            if likes < MIN_LIKES:\n",
    "                                continue\n",
    "                                \n",
    "                            # Check if post has images\n",
    "                            has_image = (hasattr(post, 'embed') and \n",
    "                                        hasattr(post.embed, 'images') and \n",
    "                                        post.embed.images and \n",
    "                                        len(post.embed.images) > 0)\n",
    "                            \n",
    "                            if has_image:\n",
    "                                try:\n",
    "                                    # Extract image URL\n",
    "                                    image_url = extract_image_url(post.embed.images[0])\n",
    "                                    \n",
    "                                    if image_url:\n",
    "                                        # Get alt text\n",
    "                                        alt_text = \"\"\n",
    "                                        if hasattr(post.embed.images[0], 'alt'):\n",
    "                                            alt_text = post.embed.images[0].alt\n",
    "                                        \n",
    "                                        # Extract data for analysis\n",
    "                                        post_data = {\n",
    "                                            'platform': 'bluesky',\n",
    "                                            'meme_template': template,\n",
    "                                            'search_query': search_query,\n",
    "                                            'post_id': post.uri,\n",
    "                                            'author_handle': post.author.handle,\n",
    "                                            'author_display': getattr(post.author, 'display_name', ''),\n",
    "                                            'caption': getattr(post.record, 'text', ''),\n",
    "                                            'alt_text': alt_text,\n",
    "                                            'image_url': image_url,\n",
    "                                            'likes': likes,\n",
    "                                            'reposts': getattr(post, 'repost_count', 0),\n",
    "                                            'replies': getattr(post, 'reply_count', 0),\n",
    "                                            'created_at': created_raw,\n",
    "                                            'indexed_at': getattr(post, 'indexed_at', ''),\n",
    "                                            'post_url': f\"https://bsky.app/profile/{post.author.handle}/post/{post.uri.split('/')[-1]}\",\n",
    "                                            'collection_timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                                        }\n",
    "                                        \n",
    "                                        template_posts.append(post_data)\n",
    "                                        unique_post_ids.add(post.uri)\n",
    "                                        page_collected += 1\n",
    "                                        total_collected_query += 1\n",
    "                                \n",
    "                                except Exception as e:\n",
    "                                    print(f\"       Error processing image: {e}\")\n",
    "                                    continue\n",
    "                        \n",
    "                        print(f\"      Page {page_num}: Found {len(feed.posts)} posts, {page_collected} kept\")\n",
    "                        \n",
    "                        # Check for next page within this time window\n",
    "                        if hasattr(feed, 'cursor') and feed.cursor:\n",
    "                            cursor = feed.cursor\n",
    "                            page_num += 1\n",
    "                            time.sleep(0.3)  # Rate limiting\n",
    "                        else:\n",
    "                            break # No cursor, done with this window\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        print(f\"       API Error in window {window_start.date()}: {e}\")\n",
    "                        time.sleep(2) # Error backoff\n",
    "                        break\n",
    "                \n",
    "                # Move to next time window\n",
    "                window_start = window_end\n",
    "                time.sleep(0.5) # Courtesy sleep between windows\n",
    "            \n",
    "            print(f\"    Total from this query: {total_collected_query} new meme posts\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    Error with query '{search_query}': {e}\")\n",
    "            continue\n",
    "        \n",
    "        time.sleep(1)  # Rate limiting between queries\n",
    "    \n",
    "    # Add to overall collection\n",
    "    all_meme_data.extend(template_posts)\n",
    "    print(f\" Total for '{template}': {len(template_posts)} meme posts\")\n",
    "    \n",
    "    # Save intermediate results\n",
    "    if template_posts:\n",
    "        template_df = pd.DataFrame(template_posts)\n",
    "        # Clean filename\n",
    "        safe_template_name = \"\".join([c for c in template if c.isalpha() or c.isdigit() or c==' ']).rstrip()\n",
    "        template_file = PROCESSED_DIR / f\"memes_{safe_template_name.replace(' ', '_')}_{datetime.now().strftime('%Y%m%d_%H%M')}.csv\"\n",
    "        template_df.to_csv(template_file, index=False, encoding='utf-8')\n",
    "        print(f\" Saved template data to: {template_file}\")\n",
    "# Create final DataFrame\n",
    "if all_meme_data:\n",
    "    df_comprehensive = pd.DataFrame(all_meme_data)\n",
    "\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\" COLLECTION COMPLETE!\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Total meme posts collected: {len(df_comprehensive)}\")\n",
    "    print(f\"Unique posts: {len(unique_post_ids)}\")\n",
    "    \n",
    "    # Save comprehensive dataset\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "    output_file = PROCESSED_DIR / f\"all_memes_comprehensive_{timestamp}.csv\"\n",
    "    df_comprehensive.to_csv(output_file, index=False, encoding='utf-8')\n",
    "    \n",
    "    print(f\"\\n SAVED COMPREHENSIVE DATASET:\")\n",
    "    print(f\"   {output_file}\")\n",
    "    \n",
    "    # Show statistics\n",
    "    print(f\"\\n BREAKDOWN BY TEMPLATE:\")\n",
    "    template_counts = df_comprehensive['meme_template'].value_counts()\n",
    "    for template, count in template_counts.items():\n",
    "        print(f\"   {template}: {count} posts\")\n",
    "    \n",
    "    print(f\"\\n ENGAGEMENT STATISTICS:\")\n",
    "    print(f\"   Average likes: {df_comprehensive['likes'].mean():.1f}\")\n",
    "    print(f\"   Average reposts: {df_comprehensive['reposts'].mean():.1f}\")\n",
    "    print(f\"   Average replies: {df_comprehensive['replies'].mean():.1f}\")\n",
    "    \n",
    "    print(f\"\\n SAMPLE DATA (first 3 rows):\")\n",
    "    print(df_comprehensive[['meme_template', 'author_handle', 'caption', 'likes', 'reposts']].head(3).to_string(index=False))\n",
    "    \n",
    "    # Additional analysis\n",
    "    print(f\"\\n ADDITIONAL INSIGHTS:\")\n",
    "    print(f\"   Posts with images: {len(df_comprehensive)}\")\n",
    "    print(f\"   Unique authors: {df_comprehensive['author_handle'].nunique()}\")\n",
    "    print(f\"   Date range: {df_comprehensive['created_at'].min()} to {df_comprehensive['created_at'].max()}\")\n",
    "    \n",
    "else: No data collected. Need to adjust search strategy.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57d4f16",
   "metadata": {},
   "source": [
    "## to be deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c479e18-f247-4b44-8d39-908f95e4cc1c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 3: Download images for all collected memes\n",
    "def download_all_meme_images(df):\n",
    "    \"\"\"Download images for comprehensive dataset\"\"\"\n",
    "    import requests\n",
    "    from PIL import Image\n",
    "    import io\n",
    "    \n",
    "    print(\"\\nðŸ“¥ DOWNLOADING ALL MEME IMAGES\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Create images directory\n",
    "    IMAGES_DIR = PROJECT_ROOT / \"images\"\n",
    "    IMAGES_DIR.mkdir(exist_ok=True)\n",
    "    \n",
    "    download_count = 0\n",
    "    error_count = 0\n",
    "    df = df.copy()  # Work on a copy\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        try:\n",
    "            # Skip if no image URL\n",
    "            if not row['image_url'] or pd.isna(row['image_url']):\n",
    "                error_count += 1\n",
    "                continue\n",
    "            \n",
    "            # Create organized folder structure\n",
    "            template_folder = row['meme_template'].replace(' ', '_').lower()[:20]\n",
    "            template_dir = IMAGES_DIR / template_folder\n",
    "            template_dir.mkdir(exist_ok=True)\n",
    "            \n",
    "            # Create descriptive filename\n",
    "            author_safe = row['author_handle'].replace('.bsky.social', '').replace('.', '_')[:15]\n",
    "            date_part = row['created_at'][:10] if row['created_at'] else 'unknown'\n",
    "            filename = f\"{template_folder}_{author_safe}_{date_part}_{idx}.jpg\"\n",
    "            filepath = template_dir / filename\n",
    "            \n",
    "            # Skip if already downloaded\n",
    "            if filepath.exists():\n",
    "                df.at[idx, 'image_path'] = str(filepath)\n",
    "                download_count += 1\n",
    "                continue\n",
    "            \n",
    "            # Download image\n",
    "            print(f\"  Downloading {template_folder}/{filename}...\")\n",
    "            \n",
    "            headers = {\n",
    "                'User-Agent': 'Meme Research Project v1.0 (Academic)',\n",
    "                'Accept': 'image/*'\n",
    "            }\n",
    "            \n",
    "            response = requests.get(row['image_url'], headers=headers, timeout=20)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                # Save image\n",
    "                with open(filepath, 'wb') as f:\n",
    "                    f.write(response.content)\n",
    "                \n",
    "                # Verify it's a valid image\n",
    "                try:\n",
    "                    img = Image.open(filepath)\n",
    "                    img.verify()  # Verify it's a valid image\n",
    "                    \n",
    "                    # Store metadata\n",
    "                    df.at[idx, 'image_path'] = str(filepath)\n",
    "                    df.at[idx, 'image_width'], df.at[idx, 'image_height'] = img.size\n",
    "                    df.at[idx, 'image_format'] = img.format\n",
    "                    \n",
    "                    download_count += 1\n",
    "                    print(f\"    âœ“ {img.size} ({img.format})\")\n",
    "                    \n",
    "                except Exception as img_error:\n",
    "                    print(f\"    âš ï¸ Invalid image: {img_error}\")\n",
    "                    filepath.unlink(missing_ok=True)\n",
    "                    error_count += 1\n",
    "            \n",
    "            else:\n",
    "                print(f\"    âŒ HTTP {response.status_code}\")\n",
    "                error_count += 1\n",
    "            \n",
    "            # Rate limiting\n",
    "            time.sleep(0.2)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  âŒ Error row {idx}: {str(e)[:100]}\")\n",
    "            error_count += 1\n",
    "    \n",
    "    return df, download_count, error_count\n",
    "\n",
    "# Download images if we have data\n",
    "if 'df_comprehensive' in locals() and not df_comprehensive.empty:\n",
    "    print(f\"\\nðŸ–¼ï¸  Starting image download for {len(df_comprehensive)} posts...\")\n",
    "    \n",
    "    df_with_images, downloaded, errors = download_all_meme_images(df_comprehensive)\n",
    "    \n",
    "    print(f\"\\nðŸ“Š IMAGE DOWNLOAD SUMMARY:\")\n",
    "    print(f\"   Successfully downloaded: {downloaded}\")\n",
    "    print(f\"   Errors: {errors}\")\n",
    "    print(f\"   Success rate: {downloaded/(downloaded+errors)*100:.1f}%\")\n",
    "    \n",
    "    # Save dataset with image paths\n",
    "    final_output = PROCESSED_DIR / f\"all_memes_with_images_{timestamp}.csv\"\n",
    "    df_with_images.to_csv(final_output, index=False, encoding='utf-8')\n",
    "    \n",
    "    print(f\"\\nðŸ’¾ FINAL DATASET SAVED TO:\")\n",
    "    print(f\"   {final_output}\")\n",
    "    \n",
    "    # Create a summary report\n",
    "    summary_file = PROCESSED_DIR / f\"collection_summary_{timestamp}.txt\"\n",
    "    with open(summary_file, 'w') as f:\n",
    "        f.write(\"MEME DATA COLLECTION SUMMARY\\n\")\n",
    "        f.write(\"=\" * 40 + \"\\n\\n\")\n",
    "        f.write(f\"Collection date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(f\"Total posts collected: {len(df_with_images)}\\n\")\n",
    "        f.write(f\"Images successfully downloaded: {downloaded}\\n\")\n",
    "        f.write(f\"Download errors: {errors}\\n\\n\")\n",
    "        \n",
    "        f.write(\"BY TEMPLATE:\\n\")\n",
    "        for template, count in df_with_images['meme_template'].value_counts().items():\n",
    "            f.write(f\"  {template}: {count}\\n\")\n",
    "    \n",
    "    print(f\"\\nðŸ“„ Summary report saved to: {summary_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44059b5",
   "metadata": {},
   "source": [
    "## 3. Load processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b9eb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# In the final project, you will provide cleaned CSV files in data/processed.\n",
    "# Here we load them; if they do not exist yet, we show an empty DataFrame\n",
    "# so that the notebook can still run without errors.\n",
    "\n",
    "reddit_path = PROCESSED_DIR / \"reddit_memes.csv\"\n",
    "bluesky_path = PROCESSED_DIR / \"bluesky_memes.csv\"\n",
    "\n",
    "if reddit_path.exists():\n",
    "    reddit_df = pd.read_csv(reddit_path)\n",
    "else:\n",
    "    reddit_df = pd.DataFrame()\n",
    "    print(\"WARNING: reddit_memes.csv not found in data/processed. Using empty DataFrame.\")\n",
    "\n",
    "if bluesky_path.exists():\n",
    "    bluesky_df = pd.read_csv(bluesky_path)\n",
    "else:\n",
    "    bluesky_df = pd.DataFrame()\n",
    "    print(\"WARNING: bluesky_memes.csv not found in data/processed. Using empty DataFrame.\")\n",
    "\n",
    "print(\"Reddit shape:\", reddit_df.shape)\n",
    "print(\"Bluesky shape:\", bluesky_df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a9bef9",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Data cleaning and harmonization\n",
    "\n",
    "Goals:\n",
    "\n",
    "- Merge Reddit and Bluesky into a unified schema.  \n",
    "- Standardize timestamp format.  \n",
    "- Create unified fields for:\n",
    "  - `caption` (title/text)  \n",
    "  - `image_url`  \n",
    "  - `likes`, `comments`, `shares` (or platform equivalents)  \n",
    "  - `platform`  \n",
    "\n",
    "We also filter to posts that actually contain images and belong to the meme templates of interest.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606878bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example harmonization skeleton\n",
    "\n",
    "def unify_reddit(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if df.empty:\n",
    "        return df\n",
    "    out = df.copy()\n",
    "    out[\"platform\"] = \"reddit\"\n",
    "    out[\"caption\"] = out[\"title\"].fillna(\"\") + \" \" + out[\"selftext\"].fillna(\"\")\n",
    "    out[\"image_url\"] = out[\"url\"]\n",
    "    out[\"likes\"] = out[\"score\"]\n",
    "    out[\"comments\"] = out[\"num_comments\"]\n",
    "    out[\"shares\"] = np.nan  # Reddit has no explicit \"shares\"\n",
    "    out[\"created_at\"] = pd.to_datetime(out[\"created_utc\"], unit=\"s\")\n",
    "    return out[[\"platform\", \"caption\", \"image_url\", \"likes\", \"comments\", \"shares\", \"created_at\"]]\n",
    "\n",
    "def unify_bluesky(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if df.empty:\n",
    "        return df\n",
    "    out = df.copy()\n",
    "    out[\"platform\"] = \"bluesky\"\n",
    "    out[\"caption\"] = out[\"text\"]\n",
    "    out[\"image_url\"] = out.get(\"image_urls\", \"\")\n",
    "    out[\"likes\"] = out.get(\"like_count\", np.nan)\n",
    "    out[\"comments\"] = out.get(\"reply_count\", np.nan)\n",
    "    out[\"shares\"] = out.get(\"repost_count\", np.nan)\n",
    "    out[\"created_at\"] = pd.to_datetime(out[\"created_at\"])\n",
    "    return out[[\"platform\", \"caption\", \"image_url\", \"likes\", \"comments\", \"shares\", \"created_at\"]]\n",
    "\n",
    "unified_frames = []\n",
    "for df, fn in [(reddit_df, unify_reddit), (bluesky_df, unify_bluesky)]:\n",
    "    unified_frames.append(fn(df))\n",
    "\n",
    "full_df = pd.concat(unified_frames, ignore_index=True) if unified_frames else pd.DataFrame()\n",
    "print(\"Unified dataset shape:\", full_df.shape)\n",
    "full_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6de578f",
   "metadata": {},
   "source": [
    "## 5. Calculate engagement metric + graphs (monthly)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479efe3c",
   "metadata": {},
   "source": [
    "### 5.1 Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbe89d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"merged.csv\")\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Clean & convert data types\n",
    "# -----------------------------\n",
    "\n",
    "# Ensure numeric types\n",
    "df[\"likes\"] = pd.to_numeric(df[\"likes\"], errors=\"coerce\")\n",
    "df[\"replies\"] = pd.to_numeric(df[\"replies\"], errors=\"coerce\")\n",
    "\n",
    "# Convert created_at to datetime\n",
    "df[\"created_at\"] = pd.to_datetime(df[\"created_at\"], errors=\"coerce\")\n",
    "\n",
    "# Extract Year-Month (YYYY-MM)\n",
    "df[\"month\"] = df[\"created_at\"].dt.to_period(\"M\").astype(str)\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Compute Weighted Engagement\n",
    "# -----------------------------\n",
    "df[\"engagement\"] = df[\"likes\"] + 2 * df[\"replies\"]\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Group by Template Ã— Month Ã— Platform\n",
    "# -----------------------------\n",
    "monthly = (\n",
    "    df.groupby([\"platform\", \"meme_template\", \"month\"])\n",
    "      .agg(\n",
    "          total_engagement = (\"engagement\", \"sum\"),\n",
    "          avg_engagement   = (\"engagement\", \"mean\"),\n",
    "          post_count       = (\"engagement\", \"count\"),\n",
    "          total_likes      = (\"likes\", \"sum\"),\n",
    "          total_replies    = (\"replies\", \"sum\")\n",
    "      )\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Save monthly summary\n",
    "# -----------------------------\n",
    "monthly.to_csv(\"monthly_template_summary.csv\", index=False)\n",
    "\n",
    "monthly.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf6cf89",
   "metadata": {},
   "source": [
    "### 5.2 Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68565e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load your monthly summary file\n",
    "monthly = pd.read_csv(\"monthly_template_summary.csv\")\n",
    "\n",
    "# Ensure month is sorted correctly\n",
    "monthly[\"month\"] = pd.to_datetime(monthly[\"month\"])\n",
    "\n",
    "# Get the unique templates\n",
    "templates = monthly[\"meme_template\"].unique()\n",
    "\n",
    "# Loop through templates and create a plot for each one\n",
    "for template in templates:\n",
    "    subset = monthly[monthly[\"meme_template\"] == template].sort_values(\"month\")\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    # Plot separately for each platform\n",
    "    for platform in subset[\"platform\"].unique():\n",
    "        platform_data = subset[subset[\"platform\"] == platform]\n",
    "        plt.plot(\n",
    "            platform_data[\"month\"],\n",
    "            platform_data[\"total_engagement\"],\n",
    "            marker=\"o\",\n",
    "            label=platform,\n",
    "        )\n",
    "\n",
    "    plt.title(f\"Monthly Engagement Timeline: {template}\")\n",
    "    plt.xlabel(\"Month\")\n",
    "    plt.ylabel(\"Total Engagement (likes + 2 Ã— replies)\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save each figure\n",
    "    plt.savefig(f\"timeline_{template.replace(' ', '_')}.png\", dpi=300)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3e10fa",
   "metadata": {},
   "source": [
    "\n",
    "## 12. Limitations and ethical considerations\n",
    "\n",
    "In this markdown cell, discuss:\n",
    "\n",
    "- Sampling biases (which subreddits / hashtags you used)  \n",
    "- API limitations (rate limits, deleted posts, etc.)  \n",
    "- Representativeness of Reddit and Bluesky users  \n",
    "- Potential ethical issues (e.g., sensitive content, privacy)  \n",
    "- Constraints of using automated sentiment / framing models on memes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd648345",
   "metadata": {},
   "source": [
    "\n",
    "## 13. Summary and link to paper\n",
    "\n",
    "Summarize the main findings that this notebook reproduces, in a few bullet points:\n",
    "\n",
    "- How visual templates relate to virality  \n",
    "- How caption framing and sentiment relate to virality  \n",
    "- Key differences between Reddit and Bluesky  \n",
    "\n",
    "Reference the corresponding sections / figures in your paper so graders can easily cross-check.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
